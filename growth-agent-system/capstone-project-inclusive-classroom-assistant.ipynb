{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6d2775",
   "metadata": {
    "papermill": {
     "duration": 0.012898,
     "end_time": "2025-04-17T13:48:56.796941",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.784043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GenAI Capstone Project 2025Q1\n",
    "# ğŸ“ GenAI Meets Education: The Inclusive Classroom Assistant ğŸ“š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98454740",
   "metadata": {
    "papermill": {
     "duration": 0.011448,
     "end_time": "2025-04-17T13:48:56.846895",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.835447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Objectives\n",
    "Accessibility is the main goal of this project. Students who are deaf or have hearing loss frequently rely on manually written notes or interpreters, neither of which are always available or reliable. By automatically converting lecture audio to text, this tool fills that gap and transforms spoken content into an engaging, reusable learning tool. The classroom becomes more inclusive and self-paced when students can **search lecture material, ask questions**, as well as even **create quizzes** for self-testing after it has been transcribed.\n",
    "\n",
    "The following sections show the step-by-step code implementation of these features. It starts with setting up the environment, installing required libraries and packages, as well as building each component under the key features such as transcription, indexing, retrieval, and quiz generation. Every function plays a specific role in achieving the overall objective. In doing this, the lecture content becomes more inclusive, searchable, and engaging for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353b552",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.011422,
     "end_time": "2025-04-17T13:48:56.893576",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.882154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# README File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022b1005",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:48:56.919853Z",
     "iopub.status.busy": "2025-04-17T13:48:56.919318Z",
     "iopub.status.idle": "2025-04-17T13:48:56.926472Z",
     "shell.execute_reply": "2025-04-17T13:48:56.925547Z"
    },
    "papermill": {
     "duration": 0.022453,
     "end_time": "2025-04-17T13:48:56.928193",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.905740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %%writefile README.md\n",
    "# # Inclusive Classroom Assistant\n",
    "#\n",
    "# ## Project Objective\n",
    "# The **Inclusive Classroom Assistant** is designed to help educators and learners interact with lecture content more effectively. It leverages various Gen AI capabilities (audio transcription, retrieval augmented generation, quiz generation, etc.) to process lectures and generate actionable insights.\n",
    "#\n",
    "# Inclusive Classroom Assistant\n",
    "# Project Objective\n",
    "# The Inclusive Classroom Assistant is designed to help educators and learners interact with lecture content more effectively. The project integrates multiple Gen AI capabilities to:\n",
    "#\n",
    "# Transcribe audio lectures: Convert uploaded audio files into text using Gemini STT.\n",
    "#\n",
    "# Index and search transcripts: Use embeddings and a vector database (Chroma) to semantically index the transcript.\n",
    "#\n",
    "# Answer questions: Employ a Retrieval Augmented Generation (RAG) chain to answer questions based solely on the transcript context.\n",
    "#\n",
    "# Generate quizzes: Create multiple-choice quiz questions from the transcript using few-shot prompting and structured JSON output.\n",
    "#\n",
    "# How to Run It\n",
    "# Set Up Your Environment:\n",
    "#\n",
    "# Ensure you have your Kaggle Notebook environment configured.\n",
    "#\n",
    "# Install any required dependencies (e.g., LangChain, Chroma, GoogleGenAI modules) via pip in your notebook cell:\n",
    "#\n",
    "# bash\n",
    "# Copy\n",
    "# !pip install langchain chromadb google-generative-ai\n",
    "# Set your necessary API keys (e.g., GOOGLE_API_KEY) in the Notebookâ€™s environment variables.\n",
    "#\n",
    "# Run Cells Sequentially:\n",
    "#\n",
    "# Start by running the ORAGANIZED code\n",
    "#\n",
    "# Finally, run the main notebook cell that integrates the Gradio interface. Follow the on-screen instructions to upload audio, transcribe it, index the transcript, ask questions, and generate quizzes.\n",
    "#\n",
    "# Interacting with the Interface:\n",
    "#\n",
    "# Upload an audio file in the Transcription & Indexing tab to get a transcript.\n",
    "#\n",
    "# Use the Query Lecture Content tab to ask questions based on the lecture.\n",
    "#\n",
    "# Navigate to the Quiz Generator tab to generate and take quizzes derived from the lecture transcript.\n",
    "#\n",
    "# Key Features\n",
    "# Audio Transcription:\n",
    "# Use Geminiâ€™s speech-to-text engine to convert audio lectures into text.\n",
    "#\n",
    "# Transcript Indexing:\n",
    "# Break the transcript into chunks and store them in a vector database using embeddings.\n",
    "#\n",
    "# Question Answering:\n",
    "# Apply a RAG chain that retrieves context and uses few-shot prompting to generate answers directly from the transcript.\n",
    "#\n",
    "# Quiz Generation:\n",
    "# Generate multiple-choice quizzes with controlled JSON output and few-shot examples for consistency.\n",
    "#\n",
    "# User-Friendly Gradio Interface:\n",
    "# An interactive UI with custom retro styling, featuring neon and pixel fonts for a unique, gamified experience.\n",
    "#\n",
    "# Gen AI Capabilities and Their Implementation\n",
    "# The project integrates several Gen AI capabilities, which are implemented as follows:\n",
    "#\n",
    "# Structured Output / JSON Mode / Controlled Generation\n",
    "#\n",
    "# Where: In the quiz-generation chain.\n",
    "#\n",
    "# How: The quiz prompt instructs the LLM to output valid JSON in a strict format (question, options, answer). A JSON parser (using JsonOutputParser) is then applied to convert the output for further processing.\n",
    "#\n",
    "# Few-Shot Prompting\n",
    "#\n",
    "# Where: In the RAG chainâ€™s main prompt and the quiz prompt.\n",
    "#\n",
    "# How: The main prompt includes few-shot examples (e.g., examples regarding Master Sito) to guide the model on how to answer questions based on transcript excerpts. The quiz prompt also incorporates examples to demonstrate the expected structure for quiz questions.\n",
    "#\n",
    "# Audio Understanding\n",
    "#\n",
    "# Where: In the Gemini STT function (transcribe_audio_chunk).\n",
    "#\n",
    "# How: The code uploads an audio file to Gemini and retrieves a text transcription of the lecture.\n",
    "#\n",
    "# Function Calling\n",
    "#\n",
    "# Where: Standard function calls are used to handle transcription, indexing, RAG chain setup, and quiz generation.\n",
    "#\n",
    "# How: Although not using dynamic LLM function calls, the code is structured modularly to call functions based on user interaction in the Gradio interface.\n",
    "#\n",
    "# Grounding\n",
    "#\n",
    "# Where: In the RAG chain prompt.\n",
    "#\n",
    "# How: The prompt instructs the LLM to base its answers exclusively on the transcript context, disregarding minor transcription errors and ensuring the answer is grounded in the provided text.\n",
    "#\n",
    "# Embeddings\n",
    "#\n",
    "# Where: During the creation of the vector database in create_vector_db.\n",
    "#\n",
    "# How: The code uses GoogleGenerativeAIEmbeddings to convert transcript chunks into vector representations, which are then stored in Chroma.\n",
    "#\n",
    "# Retrieval Augmented Generation (RAG)\n",
    "#\n",
    "# Where: In the RAG chain setup (within setup_rag_chain).\n",
    "#\n",
    "# How: The chain combines a retriever (via MultiQueryRetriever) and a ChatGPT-like LLM to generate answers based on context retrieved from the vector store.\n",
    "#\n",
    "# Vector Search / Vector Store / Vector Database\n",
    "#\n",
    "# Where: During transcript chunk indexing with create_vector_db.\n",
    "#\n",
    "# How: The transcript chunks are embedded and stored in a vector database (Chroma), enabling semantic searches that support the RAG chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c0224",
   "metadata": {
    "papermill": {
     "duration": 0.011474,
     "end_time": "2025-04-17T13:48:56.952164",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.940690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install Libraries & Packages\n",
    "In the code cell below, some important libraries and packages are installed to be able to run thisâ€‚notebook.  These include tools to access Googleâ€™s Generative AI models, creating interactive user interfaces with Gradio, as well as building smart applications with LangChain. On the other hand, ChromaDB is used to store and search text efficiently, whereas FastAPI is included to enable deployment of the project as a web service if needed. Byâ€‚installing these libraries and packages, it will ensure that all components of the AI assistant work properly within the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0772837b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:48:56.977118Z",
     "iopub.status.busy": "2025-04-17T13:48:56.976829Z",
     "iopub.status.idle": "2025-04-17T13:50:16.776874Z",
     "shell.execute_reply": "2025-04-17T13:50:16.775771Z"
    },
    "executionInfo": {
     "elapsed": 74136,
     "status": "ok",
     "timestamp": 1744766484753,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "LXxWKQ9ZoyYK",
    "outputId": "ab2104b9-0039-414f-b80e-9200e821e656",
    "papermill": {
     "duration": 79.814691,
     "end_time": "2025-04-17T13:50:16.778761",
     "exception": false,
     "start_time": "2025-04-17T13:48:56.964070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.7/159.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting langchain_community\r\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain_community)\r\n",
      "  Downloading langchain_core-0.3.53-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting langchain<1.0.0,>=0.3.23 (from langchain_community)\r\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.16)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\r\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\r\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.8)\r\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\r\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.2.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\r\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.23->langchain_community)\r\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (2.11.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.13.1)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\r\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2.4.1)\r\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\r\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.4.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain_community) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\r\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\r\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.3.53-py3-none-any.whl (433 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m433.3/433.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\r\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\r\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\r\n",
      "Installing collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain_community\r\n",
      "  Attempting uninstall: langchain-core\r\n",
      "    Found existing installation: langchain-core 0.3.35\r\n",
      "    Uninstalling langchain-core-0.3.35:\r\n",
      "      Successfully uninstalled langchain-core-0.3.35\r\n",
      "  Attempting uninstall: langchain-text-splitters\r\n",
      "    Found existing installation: langchain-text-splitters 0.3.6\r\n",
      "    Uninstalling langchain-text-splitters-0.3.6:\r\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.6\r\n",
      "  Attempting uninstall: langchain\r\n",
      "    Found existing installation: langchain 0.3.18\r\n",
      "    Uninstalling langchain-0.3.18:\r\n",
      "      Successfully uninstalled langchain-0.3.18\r\n",
      "Successfully installed httpx-sse-0.4.0 langchain-0.3.23 langchain-core-0.3.53 langchain-text-splitters-0.3.8 langchain_community-0.3.21 pydantic-settings-2.8.1 python-dotenv-1.1.0\r\n",
      "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-genai 1.11.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\r\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-generativeai 0.3.2 requires google-ai-generativelanguage==0.4.0, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mCollecting chromadb\r\n",
      "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\r\n",
      "Collecting build>=1.0.3 (from chromadb)\r\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.3)\r\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\r\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\r\n",
      "Collecting fastapi==0.115.9 (from chromadb)\r\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\r\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\r\n",
      "Collecting posthog>=2.4.0 (from chromadb)\r\n",
      "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\r\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\r\n",
      "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\r\n",
      "Collecting pypika>=0.48.9 (from chromadb)\r\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\r\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\r\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\r\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\r\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\r\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\r\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\r\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\r\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (14.0.0)\r\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\r\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\r\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\r\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\r\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\r\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\r\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\r\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\r\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\r\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\r\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\r\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\r\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\r\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\r\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\r\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\r\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\r\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\r\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\r\n",
      "Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\r\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\r\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\r\n",
      "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\r\n",
      "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\r\n",
      "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\r\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\r\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\r\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\r\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\r\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\r\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=f8d84f14701906d88b50a682386b51d8af8b8daa28575a5e64f770eb1b29f397\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\r\n",
      "Successfully built pypika\r\n",
      "Installing collected packages: pypika, monotonic, durationpy, uvloop, pyproject_hooks, protobuf, opentelemetry-util-http, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, onnxruntime, chroma-hnswlib, chromadb\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: starlette\r\n",
      "    Found existing installation: starlette 0.46.2\r\n",
      "    Uninstalling starlette-0.46.2:\r\n",
      "      Successfully uninstalled starlette-0.46.2\r\n",
      "  Attempting uninstall: opentelemetry-api\r\n",
      "    Found existing installation: opentelemetry-api 1.16.0\r\n",
      "    Uninstalling opentelemetry-api-1.16.0:\r\n",
      "      Successfully uninstalled opentelemetry-api-1.16.0\r\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\r\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\r\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\r\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\r\n",
      "  Attempting uninstall: fastapi\r\n",
      "    Found existing installation: fastapi 0.115.12\r\n",
      "    Uninstalling fastapi-0.115.12:\r\n",
      "      Successfully uninstalled fastapi-0.115.12\r\n",
      "  Attempting uninstall: opentelemetry-sdk\r\n",
      "    Found existing installation: opentelemetry-sdk 1.16.0\r\n",
      "    Uninstalling opentelemetry-sdk-1.16.0:\r\n",
      "      Successfully uninstalled opentelemetry-sdk-1.16.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-generativeai 0.3.2 requires google-ai-generativelanguage==0.4.0, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 posthog-3.25.0 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.45.3 uvloop-0.21.0 watchfiles-1.0.5\r\n",
      "Collecting fastapi==0.112.2\r\n",
      "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi==0.112.2)\r\n",
      "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.112.2) (2.11.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.112.2) (4.13.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.112.2) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.112.2) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.112.2) (0.4.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi==0.112.2) (4.9.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi==0.112.2) (3.10)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi==0.112.2) (1.3.1)\r\n",
      "Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: starlette, fastapi\r\n",
      "  Attempting uninstall: starlette\r\n",
      "    Found existing installation: starlette 0.45.3\r\n",
      "    Uninstalling starlette-0.45.3:\r\n",
      "      Successfully uninstalled starlette-0.45.3\r\n",
      "  Attempting uninstall: fastapi\r\n",
      "    Found existing installation: fastapi 0.115.9\r\n",
      "    Uninstalling fastapi-0.115.9:\r\n",
      "      Successfully uninstalled fastapi-0.115.9\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "chromadb 1.0.5 requires fastapi==0.115.9, but you have fastapi 0.112.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed fastapi-0.112.2 starlette-0.38.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-genai\n",
    "!pip install langchain_community\n",
    "!pip install gradio==4.14.0 google-generativeai==0.3.2 --quiet\n",
    "!pip install -qU langchain-google-genai\n",
    "!pip install chromadb\n",
    "!pip install fastapi==0.112.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99740035",
   "metadata": {
    "papermill": {
     "duration": 0.020826,
     "end_time": "2025-04-17T13:50:16.821238",
     "exception": false,
     "start_time": "2025-04-17T13:50:16.800412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c735ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:50:16.864322Z",
     "iopub.status.busy": "2025-04-17T13:50:16.863962Z",
     "iopub.status.idle": "2025-04-17T13:50:25.278687Z",
     "shell.execute_reply": "2025-04-17T13:50:25.277710Z"
    },
    "executionInfo": {
     "elapsed": 3958,
     "status": "error",
     "timestamp": 1744766488726,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "t2O0chT9p0se",
    "outputId": "5937b1a7-1b15-4158-c6d9-d3d7e8de08ac",
    "papermill": {
     "duration": 8.438596,
     "end_time": "2025-04-17T13:50:25.280496",
     "exception": false,
     "start_time": "2025-04-17T13:50:16.841900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import gradio as gr\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "from google.api_core.exceptions import GoogleAPIError, NotFound, PermissionDenied\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.colab import userdata\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fc49e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:50:25.369159Z",
     "iopub.status.busy": "2025-04-17T13:50:25.368573Z",
     "iopub.status.idle": "2025-04-17T13:50:25.380275Z",
     "shell.execute_reply": "2025-04-17T13:50:25.379462Z"
    },
    "papermill": {
     "duration": 0.036102,
     "end_time": "2025-04-17T13:50:25.381860",
     "exception": false,
     "start_time": "2025-04-17T13:50:25.345758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ['The surprising secret to speaking with confidence ( Bad audio -20 mins ).mp3', 'The Bird and the Whale (Clean-5mins).mp3', 'How to Start a Speech(Moderate noise-9mins).mp3', 'Google AI Studio in 26 Minutes(Clean-26mins).mp3']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/kaggle/input/kaggle-capstone-project-audio-library\"\n",
    "print(\"Files:\", os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f8381",
   "metadata": {
    "papermill": {
     "duration": 0.021878,
     "end_time": "2025-04-17T13:50:25.511912",
     "exception": false,
     "start_time": "2025-04-17T13:50:25.490034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Global State Management\n",
    "These variables store shared state across the notebook. They act like session memory that enables multiple parts of the app (transcription, RAG, quiz, etc.) to access and update the same data.\n",
    "\n",
    "| Variable               | Purpose                                                       |\n",
    "|------------------------|---------------------------------------------------------------|\n",
    "| full_transcript        | Holds all transcribed chunks from audio                       |\n",
    "| vector_db              | Stores the Chroma vector database for vector search           |\n",
    "| rag_chain              | Holds the Retrieval-Augmented Generation pipeline             |\n",
    "| current_correct_answer | Tracks quiz answers (used in quiz evaluation logic)           |\n",
    "\n",
    "\n",
    "These variables ensure seamless interaction between various functions, as well as to help in maintaining the assistant's overall state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e998a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:50:25.557005Z",
     "iopub.status.busy": "2025-04-17T13:50:25.556604Z",
     "iopub.status.idle": "2025-04-17T13:50:25.561958Z",
     "shell.execute_reply": "2025-04-17T13:50:25.560745Z"
    },
    "executionInfo": {
     "elapsed": 78202,
     "status": "aborted",
     "timestamp": 1744766488635,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "LYS6vjQZqCKT",
    "papermill": {
     "duration": 0.029651,
     "end_time": "2025-04-17T13:50:25.563700",
     "exception": false,
     "start_time": "2025-04-17T13:50:25.534049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------- Global State -------------\n",
    "full_transcript = []\n",
    "lecture_index = []\n",
    "vector_db = None\n",
    "rag_chain = None\n",
    "current_correct_answer = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199d0a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:50:26.215295Z",
     "iopub.status.busy": "2025-04-17T13:50:26.214915Z",
     "iopub.status.idle": "2025-04-17T13:50:26.273634Z",
     "shell.execute_reply": "2025-04-17T13:50:26.271999Z"
    },
    "executionInfo": {
     "elapsed": 78202,
     "status": "aborted",
     "timestamp": 1744766488636,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "5DR2hUwcqMwx",
    "papermill": {
     "duration": 0.085164,
     "end_time": "2025-04-17T13:50:26.276334",
     "exception": false,
     "start_time": "2025-04-17T13:50:26.191170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------- Transcript State Management -------------\n",
    "def get_full_transcript_text():\n",
    "    return \" \".join(full_transcript)\n",
    "\n",
    "def clear_transcript_data():\n",
    "    global full_transcript\n",
    "    full_transcript = []\n",
    "    print(\"Transcript data cleared.\")\n",
    "    return \"\"\n",
    "\n",
    "# ------------- Gemini STT Function -------------\n",
    "def transcribe_audio_chunk(audio_path):\n",
    "    try:\n",
    "        client = genai.Client(api_key=API_KEY)\n",
    "        uploaded_file = client.files.upload(file=audio_path)\n",
    "        print(f\"âœ… File uploaded. URI: {uploaded_file.uri}, Name: {uploaded_file.name}\")\n",
    "\n",
    "        prompt = (\n",
    "            \"Please perform speech-to-text transcription for the provided audio file. \"\n",
    "            \"Output the transcribed text followed by the key points as a numbered list. \"\n",
    "            \"Do not use any JSON formattingâ€”just return plain text.\"\n",
    "        )\n",
    "\n",
    "        print(\"ğŸš€ Sending transcription request to Gemini...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[prompt, uploaded_file]\n",
    "        )\n",
    "\n",
    "        if not response.candidates:\n",
    "            block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else \"Unknown\"\n",
    "            return f\"Transcription failed. Block Reason: {block_reason}\"\n",
    "\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate.content, 'parts') and candidate.content.parts:\n",
    "            transcript = candidate.content.parts[0].text\n",
    "            print(\"âœ… Transcription successful.\")\n",
    "            return transcript\n",
    "        else:\n",
    "            return \"Error: Failed to parse transcription response.\"\n",
    "\n",
    "    except PermissionDenied as e:\n",
    "        return f\"âŒ Permission Denied: {e.message}\"\n",
    "    except NotFound as e:\n",
    "        return f\"âŒ Resource Not Found: {e.message}\"\n",
    "    except GoogleAPIError as e:\n",
    "        return f\"âŒ API Error: {e.message}\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Unexpected Error: {str(e)}\"\n",
    "\n",
    "# ------------- Formatting the Output -------------\n",
    "def format_transcription_result(result_text):\n",
    "    return result_text\n",
    "\n",
    "# ------------- Gradio Transcription Handler -------------\n",
    "def handle_transcription_request(audio_file):\n",
    "    if audio_file is None:\n",
    "        return \"\", get_full_transcript_text(), gr.update(value=None), \"Transcription not initiated.\", \"Input declined. No audio file provided.\"\n",
    "\n",
    "    transcript_text = transcribe_audio_chunk(audio_file)\n",
    "    formatted_chunk = format_transcription_result(transcript_text)\n",
    "    full_transcript.append(formatted_chunk)\n",
    "\n",
    "    return (\n",
    "        formatted_chunk,\n",
    "        get_full_transcript_text(),\n",
    "        gr.update(value=None),\n",
    "        \"Transcription successful.\",\n",
    "        \"Input accepted. Audio file is being processed.\"\n",
    "    )\n",
    "\n",
    "def handle_clear_transcript():\n",
    "    clear_transcript_data()\n",
    "    return \"\", \"Transcript cleared.\"\n",
    "\n",
    "\n",
    "\n",
    "# ------------- Chunking, Embedding, Vector DB & RAG -------------\n",
    "\n",
    "def chunk_transcript(text, chunk_size: int = 800, overlap_size: int = 150):\n",
    "    # Optionally, you could call: text = correct_transcript_errors(text)\n",
    "    document = [Document(page_content=text)]\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap_size\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents=document)\n",
    "    print(f\"File split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def create_vector_db(text_chunks, collection_name=\"transcription-rag\"):\n",
    "    global vector_db\n",
    "    try:\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=API_KEY)\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=text_chunks,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=\"/content/chroma_db\"  # Ephemeral persist directory.\n",
    "        )\n",
    "        print(f\"Vector DB created with collection_name: {collection_name}\")\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error creating vector DB: {str(e)}\")\n",
    "\n",
    "def setup_rag_chain(vector_db):\n",
    "    if not vector_db:\n",
    "        raise ValueError(\"Vector DB not initialized!\")\n",
    "\n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        # Few-shot query rewriting prompt\n",
    "        query_prompt = PromptTemplate.from_template(\"\"\"\n",
    "            You are an AI assistant that helps rephrase queries.\n",
    "\n",
    "            Example 1:\n",
    "            Original Question: Who is Master Sito?\n",
    "            Alternative Queries:\n",
    "              1. According to the transcript, what is Master Sito's role?\n",
    "              2. What does the transcript state about Master Sito?\n",
    "              3. How is Master Sito described in the lecture?\n",
    "\n",
    "            Example 2:\n",
    "            Original Question: Who is Master Sito?\n",
    "            Even if the transcript contains a minor typo (e.g., 'Master Ceto'),\n",
    "            assume the intended name is Master Sito.\n",
    "\n",
    "            Now, given the original question: {question}\n",
    "            Generate three alternative queries:\n",
    "        \"\"\")\n",
    "\n",
    "        retriever = MultiQueryRetriever.from_llm(\n",
    "            retriever=vector_db.as_retriever(search_kwargs={\"k\": 4}),\n",
    "            llm=llm,\n",
    "            prompt=query_prompt\n",
    "        )\n",
    "\n",
    "        # Main prompt for answering with grounding and few-shot examples\n",
    "        main_template = \"\"\"\n",
    "            You are an educational assistant. Answer the user's question based solely on the transcript context provided.\n",
    "            Disregard minor transcription errors (for example, if the transcript has \"Master Ceto\" but context indicates it should be \"Master Sito\").\n",
    "            If the answer is explicitly stated, provide it exactly. Otherwise, reply \"I donâ€™t know.\"\n",
    "\n",
    "            Few-shot examples:\n",
    "            ---------------------\n",
    "            Transcript Example 1:\n",
    "            \"Master Sito said: 'Face life with humor.'\"\n",
    "            Q: What did Master Sito say about life?\n",
    "            A: Face life with humor.\n",
    "            ---------------------\n",
    "            Transcript Example 2:\n",
    "            \"According to the lecture, Master Sito is a monk living in seclusion.\"\n",
    "            Q: Who is Master Sito?\n",
    "            A: He is a monk.\n",
    "            ---------------------\n",
    "            Now, using the transcript below:\n",
    "            Transcript:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template=main_template)\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        print(\"RAG chain setup complete!\")\n",
    "        return chain\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up RAG chain: {str(e)}\")\n",
    "\n",
    "def handle_indexing_request(transcript_text):\n",
    "    global vector_db, rag_chain\n",
    "    if not transcript_text or len(transcript_text.strip()) == 0:\n",
    "        return \"âš ï¸ Transcript is empty. Please transcribe or paste something first.\"\n",
    "    try:\n",
    "        chunks = chunk_transcript(transcript_text)\n",
    "        vector_db = create_vector_db(chunks)\n",
    "        rag_chain = setup_rag_chain(vector_db)\n",
    "        return f\"âœ… Indexing complete. {len(chunks)} chunks indexed.\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Indexing failed: {str(e)}\"\n",
    "\n",
    "def query(chain, question: str):\n",
    "    if not chain:\n",
    "        print(\"RAG chain not initialized!\")\n",
    "    try:\n",
    "        return chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing query: {str(e)}\")\n",
    "\n",
    "def answer_query_using_rag(user_query):\n",
    "    global rag_chain\n",
    "    if not rag_chain:\n",
    "        return \"âš ï¸ Please index the transcript first.\"\n",
    "    try:\n",
    "        result = query(rag_chain, user_query)\n",
    "        return f\"ğŸ’¬ {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error: {str(e)}\"\n",
    "\n",
    "#-------------Quiz Generation with few shot prompting---------------------------------------------\n",
    "def setup_quiz_chain():\n",
    "    try:\n",
    "        llm_quiz = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "            # Consider setting a reasonable max_tokens limit, e.g., max_tokens=1024\n",
    "            max_tokens=None,\n",
    "            # Consider setting an explicit timeout, e.g., timeout=120\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key=API_KEY\n",
    "        )\n",
    "\n",
    "        quiz_template = \"\"\"\n",
    "            You are an educational assistant. Your task is to generate 5 multiple-choice quiz questions based only on the transcript provided below.\n",
    "            Please return the output strictly as valid JSON. Do not include any introductory text or markdown formatting around the JSON object.\n",
    "            The JSON should be a list containing 5 objects, each following this format:\n",
    "\n",
    "            {{\n",
    "              \"question\": \"Your quiz question here.\",\n",
    "              \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "              \"answer\": \"The correct option (must exactly match one of the options)\"\n",
    "            }}\n",
    "\n",
    "            Transcript:\n",
    "            {transcript}\n",
    "\n",
    "            JSON Output:\n",
    "        \"\"\" # Added \"JSON Output:\" hint and refined instructions slightly\n",
    "\n",
    "        quiz_prompt = PromptTemplate.from_template(quiz_template)\n",
    "        # For standard JSON:\n",
    "        parser = JsonOutputParser()\n",
    "\n",
    "        # Update the chain to use the JsonOutputParser\n",
    "        chain = (\n",
    "            {\"transcript\": RunnablePassthrough()}\n",
    "            | quiz_prompt\n",
    "            | llm_quiz\n",
    "            | parser # <-- Use JsonOutputParser instead of StrOutputParser\n",
    "        )\n",
    "        print(\"Quiz chain setup complete!\")\n",
    "        return chain\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up Quiz chain: {str(e)}\")\n",
    "\n",
    "\n",
    "# --- Global Quiz State ---\n",
    "quiz_state = None\n",
    "\n",
    "# --- Function to Generate Quiz ---\n",
    "def generate_quiz(transcript: str):\n",
    "    global quiz_state\n",
    "    if not transcript or transcript.strip() == \"\":\n",
    "        return \"âš ï¸ Please provide a transcript.\", [], \"No quiz generated.\"\n",
    "    try:\n",
    "        chain = setup_quiz_chain()\n",
    "        output = chain.invoke({\"transcript\": transcript})\n",
    "        print(\"DEBUG - Chain output:\", output)\n",
    "        quiz_data = output  # Already parsed JSON from JsonOutputParser.\n",
    "    except Exception as e:\n",
    "        return f\"Quiz generation failed: {str(e)}\", [], \"Error occurred.\"\n",
    "    if not quiz_data or len(quiz_data) == 0:\n",
    "        return \"âš ï¸ No quiz questions returned by the model.\", [], \"\"\n",
    "\n",
    "    # Initialize quiz state with an additional 'answered' flag.\n",
    "    quiz_state = {\n",
    "    \"questions\": quiz_data,\n",
    "    \"current_index\": 0,\n",
    "    \"score\": 0,\n",
    "    \"streak\": 0,            # New: Track consecutive correct answers.\n",
    "    \"answered\": False       # New: Flag to indicate if the current question is answered.\n",
    "}\n",
    "\n",
    "    first_question = quiz_data[0]\n",
    "    return first_question[\"question\"], first_question[\"options\"], \"\"\n",
    "\n",
    "# --- Function to Evaluate Answer (without advancing to next question) ---\n",
    "def select_answer(index: int):\n",
    "    global quiz_state\n",
    "    if not quiz_state or \"questions\" not in quiz_state:\n",
    "        return \"No quiz generated. Please generate a quiz first.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"âš ï¸\", \"Score: 0 | Streak: 0\"\n",
    "\n",
    "    # Prevent re-answering if the question was already answered.\n",
    "    if quiz_state.get(\"answered\", False):\n",
    "        current_question = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "        options = current_question.get(\"options\", [])\n",
    "        btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "        return (current_question[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "                \"You have already answered. Click 'Next Question' to continue.\",\n",
    "                f\"Score: {quiz_state.get('score', 0)} | Streak: {quiz_state.get('streak', 0)}\")\n",
    "\n",
    "    current_question = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "    options = current_question.get(\"options\", [])\n",
    "    if index >= len(options):\n",
    "        return \"Invalid option selected.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"Error\", f\"Score: {quiz_state.get('score', 0)} | Streak: {quiz_state.get('streak', 0)}\"\n",
    "\n",
    "    selected_option = options[index]\n",
    "\n",
    "    # Check answer and update score and streak.\n",
    "    if selected_option == current_question[\"answer\"]:\n",
    "        feedback = \"Correct!\"\n",
    "        quiz_state[\"score\"] += 1\n",
    "        quiz_state[\"streak\"] += 1\n",
    "    else:\n",
    "        feedback = f\"Incorrect. The correct answer was: {current_question['answer']}.\"\n",
    "        quiz_state[\"streak\"] = 0\n",
    "\n",
    "    quiz_state[\"answered\"] = True  # Mark the question as answered.\n",
    "    btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "    score_text = f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\"\n",
    "    return (current_question[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "            feedback, score_text)\n",
    "\n",
    "# --- Function to Advance to the Next Question ---\n",
    "def advance_to_next_question():\n",
    "    global quiz_state\n",
    "    if not quiz_state or \"questions\" not in quiz_state:\n",
    "        return \"No quiz generated. Please generate a quiz first.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"âš ï¸\", \"Score: 0 | Streak: 0\"\n",
    "\n",
    "    if not quiz_state.get(\"answered\", False):\n",
    "        return \"Please select an answer before proceeding.\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"âš ï¸\", f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\"\n",
    "\n",
    "    quiz_state[\"current_index\"] += 1\n",
    "    quiz_state[\"answered\"] = False  # Reset the answered flag.\n",
    "    if quiz_state[\"current_index\"] < len(quiz_state[\"questions\"]):\n",
    "        next_q = quiz_state[\"questions\"][quiz_state[\"current_index\"]]\n",
    "        options = next_q.get(\"options\", [])\n",
    "        btn_labels = [options[i] if i < len(options) else \"N/A\" for i in range(4)]\n",
    "        return (next_q[\"question\"], btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3],\n",
    "                \"\", f\"Score: {quiz_state['score']} | Streak: {quiz_state['streak']}\")\n",
    "    else:\n",
    "        score = quiz_state[\"score\"]\n",
    "        total = len(quiz_state[\"questions\"])\n",
    "        percentage = round((score / total) * 100)\n",
    "        color = \"red\" if percentage < 60 else \"green\"\n",
    "        # Display final score with some HTML styling.\n",
    "        percent_display = f\"<span style='color:{color}; font-weight:bold;'>{percentage}%</span>\"\n",
    "        final_msg = f\"Quiz complete! Your final score is {score} out of {total}: {percent_display}.\"\n",
    "        quiz_state = None\n",
    "        return final_msg, \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "# --- Combined function to update quiz question & button labels on generation ---\n",
    "def generate_quiz_and_buttons(transcript: str):\n",
    "    question, options, feedback = generate_quiz(transcript)\n",
    "    btn_labels = [\"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
    "    if isinstance(options, list):\n",
    "        for i in range(min(len(options), 4)):\n",
    "            btn_labels[i] = options[i]\n",
    "    score_text = \"Score: 0 | Streak: 0\"\n",
    "    return question, btn_labels[0], btn_labels[1], btn_labels[2], btn_labels[3], feedback, score_text\n",
    "\n",
    "def select_answer_and_update(index: int):\n",
    "    # (Call our select_answer function.)\n",
    "    return select_answer(index)\n",
    "\n",
    "def load_transcript(full_text):\n",
    "    # For now, simply return the same text.\n",
    "    # Adjust this function based on your intended behavior.\n",
    "    return full_text\n",
    "\n",
    "def clear_transcript():\n",
    "    # This dummy implementation clears the transcript and returns a cleared status message.\n",
    "    return \"\", \"Transcript cleared.\"\n",
    "\n",
    "def handle_query_request(user_query):\n",
    "    if not user_query or not user_query.strip():\n",
    "        return \"âš ï¸ Please enter a valid question about the lecture.\"\n",
    "\n",
    "    # Hypothetical function that uses your indexed transcript + LLM:\n",
    "    return answer_query_using_rag(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31859c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:50:26.473035Z",
     "iopub.status.busy": "2025-04-17T13:50:26.472601Z",
     "iopub.status.idle": "2025-04-17T13:50:31.634175Z",
     "shell.execute_reply": "2025-04-17T13:50:31.633301Z"
    },
    "executionInfo": {
     "elapsed": 78249,
     "status": "aborted",
     "timestamp": 1744766488686,
     "user": {
      "displayName": "Jun Loh",
      "userId": "12031303531432355173"
     },
     "user_tz": -480
    },
    "id": "sQrBXrVkqe_z",
    "papermill": {
     "duration": 5.187559,
     "end_time": "2025-04-17T13:50:31.636173",
     "exception": false,
     "start_time": "2025-04-17T13:50:26.448614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e5666a32164b98acc6c516d70e94a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "theme_schema%400.0.8.json:   0%|          | 0.00/11.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.14.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Running on public URL: https://3be454bada1e0c05f7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3be454bada1e0c05f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------ Gradio Interface with Custom Retro Theme ------------------\n",
    "with gr.Blocks(\n",
    "    theme=\"d8ahazard/material_design_rd\",\n",
    "    css=\"\"\"\n",
    "    @import url('https://fonts.cdnfonts.com/css/minecraft-4');\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap');\n",
    "\n",
    "    /* Universal (normal) font styles */\n",
    "    body,\n",
    "    .gradio-container,\n",
    "    .gr-button,\n",
    "    .gr-markdown,\n",
    "    .gr-textbox,\n",
    "    h1, h2, h3, h4, p {\n",
    "      font-family: \"Arial\", sans-serif !important;\n",
    "      color: #fff !important;\n",
    "      letter-spacing: 0.05em;\n",
    "      line-height: 1.6;\n",
    "      background-color: #101010 !important;\n",
    "      margin: 0;\n",
    "      padding: 0;\n",
    "    }\n",
    "\n",
    "    /* Accent color definition for buttons and highlights */\n",
    "    .accent-bg {\n",
    "      background-color: #8e44ad !important;  /* pink/purple background */\n",
    "      font-family: 'Press Start 2P', monospace !important;  /* pixel font */\n",
    "      color: #000 !important;                 /* black text */\n",
    "      text-shadow: none;                      /* remove neon glow */\n",
    "    }\n",
    "    .accent-bg:hover {\n",
    "      background-color: #9c59bd !important;\n",
    "    }\n",
    "\n",
    "    /* Special Minecraft heading class (for header only) */\n",
    "    .minecraft-heading {\n",
    "      font-family: 'Minecraft', sans-serif !important;\n",
    "      letter-spacing: 0.15em;\n",
    "      font-size: 28px;\n",
    "    }\n",
    "\n",
    "    /* Typewriter effect for header */\n",
    "    .typewriter {\n",
    "      overflow: hidden;\n",
    "      border-right: .15em solid #fff;\n",
    "      white-space: nowrap;\n",
    "      animation: typing 2.5s steps(30, end), blink-caret 0.75s step-end infinite;\n",
    "      width: fit-content;\n",
    "      font-weight: 700;\n",
    "      line-height: 1.8;\n",
    "    }\n",
    "    @keyframes typing {\n",
    "      from { width: 0; }\n",
    "      to { width: 100%; }\n",
    "    }\n",
    "    @keyframes blink-caret {\n",
    "      from, to { border-color: transparent; }\n",
    "      50% { border-color: #fff; }\n",
    "    }\n",
    "\n",
    "    /* Bot logo with bounce animation */\n",
    "    #bot-logo img {\n",
    "      animation: bounce 1.2s ease infinite !important;\n",
    "      border-radius: 8px;\n",
    "      width: 90px;\n",
    "      height: 90px;\n",
    "      object-fit: contain;\n",
    "      margin-right: 8px;\n",
    "    }\n",
    "    @keyframes bounce {\n",
    "      0%, 100% { transform: translateY(0); }\n",
    "      50% { transform: translateY(-8px); }\n",
    "    }\n",
    "\n",
    "    /* Dark-themed textboxes for consistency */\n",
    "    .gr-textbox, .gr-textbox textarea, .gr-textbox input {\n",
    "      background-color: #2b2b2b !important;\n",
    "      color: #fff !important;\n",
    "      border: 1px solid #555 !important;\n",
    "      border-radius: 4px;\n",
    "      padding: 4px 8px;\n",
    "    }\n",
    "    ::placeholder {\n",
    "      color: #aaa !important;\n",
    "      opacity: 1;\n",
    "    }\n",
    "\n",
    "    /* Additional spacing for header and elements */\n",
    "    #header {\n",
    "      margin-bottom: 16px;\n",
    "    }\n",
    "    .tab-content {\n",
    "      padding: 16px;\n",
    "    }\n",
    "\n",
    "    /* Retro-styled scoreboard for Quiz Generator */\n",
    "    #quiz-scoreboard {\n",
    "      border: 2px solid #0aff0a; /* Neon green border */\n",
    "      padding: 8px;\n",
    "      margin-bottom: 8px;\n",
    "      font-family: 'Press Start 2P', monospace;\n",
    "      color: #0aff0a;\n",
    "      background-color: #000;\n",
    "      text-align: right;\n",
    "    }\n",
    "\n",
    "    /* Retro panel style (for prompts or feedback) with green fonts */\n",
    "    .retro-panel {\n",
    "      border: 2px solid #ff66ff;\n",
    "      background-color: #111;\n",
    "      padding: 8px;\n",
    "      margin-bottom: 8px;\n",
    "      font-family: 'Press Start 2P', monospace;\n",
    "      color: #0aff0a !important;    /* Changed to neon green */\n",
    "      text-align: center;\n",
    "      text-shadow: 0 0 4px #0aff0a, 0 0 8px #0aff0a;\n",
    "    }\n",
    "\n",
    "    /* Neon text glow effect for headings and other elements */\n",
    "    .neon-text {\n",
    "      text-shadow: 0 0 4px #ff66ff, 0 0 8px #ff66ff;\n",
    "    }\n",
    "\n",
    "    /* Neon style for tab labels */\n",
    "    .gradio-container .tabs button {\n",
    "      font-family: 'Press Start 2P', monospace !important;\n",
    "      color: #ff66ff !important;\n",
    "      text-shadow: 0 0 4px #ff66ff, 0 0 8px #ff66ff;\n",
    "      background-color: transparent !important;\n",
    "      border: none !important;\n",
    "    }\n",
    "    .gradio-container .tabs button:hover {\n",
    "      background-color: #333 !important;\n",
    "    }\n",
    "    .gradio-container .tabs button.selected {\n",
    "      color: #0aff0a !important;\n",
    "      text-shadow: 0 0 4px #0aff0a, 0 0 8px #0aff0a;\n",
    "    }\n",
    "\n",
    "    /* --- START: Added CSS to hide Audio elements --- */\n",
    "    /* Hide the icon within the audio file drop zone */\n",
    "    .gradio-audio .file-drop svg {\n",
    "        display: none !important;\n",
    "    }\n",
    "    /* Optional: Hide the \"Drop Audio Here\" text as well */\n",
    "    /*\n",
    "    .gradio-audio .file-drop span {\n",
    "        display: none !important;\n",
    "    }\n",
    "    */\n",
    "    /* --- END: Added CSS to hide Audio elements --- */\n",
    "    \"\"\"\n",
    ") as app:\n",
    "\n",
    "    # Link Minecraft font and Press Start 2P for retro elements\n",
    "    gr.Markdown('<link href=\"https://fonts.cdnfonts.com/css/minecraft-4\" rel=\"stylesheet\">')\n",
    "    gr.Markdown('<link href=\"https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap\" rel=\"stylesheet\">')\n",
    "\n",
    "    # -- Header with Minecraft only on the H2 --\n",
    "    with gr.Row():\n",
    "        #  with gr.Column(scale=0, min_width=90):\n",
    "        #      gr.Image(value=\"/content/bot.jpg\", show_label=False, elem_id=\"bot-logo\", height=90) # Assuming bot.jpg is accessible\n",
    "         with gr.Column(scale=1):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                <h2 class=\"minecraft-heading typewriter neon-text\" style=\"margin: 0;\">\n",
    "                    Inclusive Classroom Assistant\n",
    "                </h2>\n",
    "                <p class=\"neon-text\" style=\"margin: 4px 0 0 0; font-size: 14px;\">\n",
    "                    Upload audio, transcribe, index, and ask anything about your lecture!\n",
    "                </p>\n",
    "                \"\"\",\n",
    "                elem_id=\"header\"\n",
    "            )\n",
    "\n",
    "    # ------------------ Tab 1: Transcription & Indexing ------------------\n",
    "    with gr.Tab(\"ğŸ™ï¸ Transcription & Indexing\") as tab1:\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"<h3 class='neon-text'>Transcription Input</h3>\")\n",
    "                # --- MODIFIED: Removed label text using show_label=False ---\n",
    "                audio_input = gr.Audio(type=\"filepath\", show_label=False)\n",
    "                # --- END MODIFICATION ---\n",
    "                transcribe_button = gr.Button(\"Transcribe Chunk\", elem_classes=\"accent-bg\")\n",
    "                transcription_input_status_textbox = gr.Textbox(label=\"Transcription Input Status\", lines=1, interactive=False)\n",
    "                latest_chunk_textbox = gr.Textbox(label=\"Latest Transcript Chunk\", lines=10, interactive=False)\n",
    "                status_textbox = gr.Textbox(label=\"Status\", lines=1, interactive=False)\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"<h3 class='neon-text'>Full Transcript & Indexing</h3>\")\n",
    "                full_transcript_textbox = gr.Textbox(label=\"Full Lecture Transcript\", lines=20, interactive=False)\n",
    "                with gr.Row():\n",
    "                    index_button = gr.Button(\"Index Transcript for Search\", elem_classes=\"accent-bg\")\n",
    "                    clear_button = gr.Button(\"Clear Full Transcript\", elem_classes=\"accent-bg\")\n",
    "                indexing_status_display = gr.Textbox(label=\"Indexing Status\", lines=2, interactive=False)\n",
    "\n",
    "    # ------------------ Tab 2: Query Lecture Content ------------------\n",
    "    with gr.Tab(\"ğŸ’¬ Query Lecture Content\") as tab2:\n",
    "        gr.Markdown(\"<h3 class='neon-text'>Ask a question about the lecture content</h3>\")\n",
    "        with gr.Row():\n",
    "            query_input_textbox = gr.Textbox(\n",
    "                label=\"Ask a question\",\n",
    "                placeholder=\"E.g., What lesson did Sam learn?\",\n",
    "                lines=2\n",
    "            )\n",
    "            ask_button = gr.Button(\"Ask Question\", elem_classes=\"accent-bg\")\n",
    "        # Answer display with neon and retro effects\n",
    "        answer_display = gr.Markdown(\n",
    "            \"ğŸ’¡ Answer will appear here...\",\n",
    "            elem_classes=\"query-answer-box retro-panel neon-text\",\n",
    "            # label=\"Answer\" # Markdown doesn't have a label param like this\n",
    "        )\n",
    "\n",
    "    # ------------------ Tab 3: Quiz Generator ------------------\n",
    "    with gr.Tab(\"ğŸ“ Quiz Generator\") as tab3:\n",
    "        # Scoreboard only in this tab with retro neon style\n",
    "        scoreboard = gr.Markdown(\"Score: 0 | Streak: 0\", elem_id=\"quiz-scoreboard\")\n",
    "        gr.Markdown(\"<h3 class='neon-text'>Generate Quiz from Transcript</h3>\")\n",
    "        gr.Markdown(\"<p class='retro-panel neon-text'>Click <strong>Generate Quiz</strong> to start. Answer each question and review your score and correct answer streak after each question.</p>\")\n",
    "        generate_btn = gr.Button(\"Generate Quiz\", elem_classes=\"accent-bg\")\n",
    "        quiz_question = gr.Markdown(\"Question will appear here\", elem_classes=\"retro-panel neon-text\")\n",
    "        with gr.Row():\n",
    "            option_button1 = gr.Button(\"Option 1\", elem_classes=\"accent-bg\")\n",
    "            option_button2 = gr.Button(\"Option 2\", elem_classes=\"accent-bg\")\n",
    "            option_button3 = gr.Button(\"Option 3\", elem_classes=\"accent-bg\")\n",
    "            option_button4 = gr.Button(\"Option 4\", elem_classes=\"accent-bg\")\n",
    "        feedback_box = gr.Textbox(label=\"Feedback\", interactive=False, elem_classes=\"retro-panel neon-text\")\n",
    "        next_btn = gr.Button(\"Next Question\", elem_classes=\"accent-bg\")\n",
    "\n",
    "    # ------------------ Button Callback Bindings (Placeholder - Add your actual functions) ------------------\n",
    "\n",
    "    transcribe_button.click(\n",
    "        fn=handle_transcription_request,\n",
    "        inputs=[audio_input],\n",
    "        outputs=[latest_chunk_textbox, full_transcript_textbox, audio_input, status_textbox, transcription_input_status_textbox]\n",
    "    )\n",
    "    index_button.click(\n",
    "        fn=handle_indexing_request,\n",
    "        inputs=[full_transcript_textbox],\n",
    "        outputs=[indexing_status_display]\n",
    "    )\n",
    "    clear_button.click(\n",
    "        fn=clear_transcript_data,\n",
    "        inputs=None,\n",
    "        outputs=[full_transcript_textbox, status_textbox]\n",
    "    )\n",
    "    ask_button.click(\n",
    "        fn=handle_query_request,\n",
    "        inputs=[query_input_textbox],\n",
    "        outputs=[answer_display]\n",
    "    )\n",
    "    generate_btn.click(\n",
    "        fn=generate_quiz_and_buttons,\n",
    "        inputs=[full_transcript_textbox],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button1.click(\n",
    "        fn=lambda: select_answer_and_update(0),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button2.click(\n",
    "        fn=lambda: select_answer_and_update(1),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button3.click(\n",
    "        fn=lambda: select_answer_and_update(2),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    option_button4.click(\n",
    "        fn=lambda: select_answer_and_update(3),\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "    next_btn.click(\n",
    "        fn=advance_to_next_question,\n",
    "        inputs=[],\n",
    "        outputs=[quiz_question, option_button1, option_button2, option_button3, option_button4, feedback_box, scoreboard]\n",
    "    )\n",
    "\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ed144",
   "metadata": {
    "papermill": {
     "duration": 0.021583,
     "end_time": "2025-04-17T13:50:31.679993",
     "exception": false,
     "start_time": "2025-04-17T13:50:31.658410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deployment\n",
    "This Inclusive Classroom Assistance is later deployed to **Hugging Face** with the link below:\n",
    "[ https://huggingface.co/spaces/MonaHamid/inclusive-classroom-assistant](http://)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 7157702,
     "sourceId": 11428326,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7167704,
     "sourceId": 11442058,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 102.541755,
   "end_time": "2025-04-17T13:50:34.537673",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-17T13:48:51.995918",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2ee9bad19f0249d58d69eae77819bf86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3a323e3fe9db452b9d3d7c042588b09b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9f5ab752c521431a9f645b676b07e49f",
       "max": 11487,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b83a590d21b4eff8d2286ac7126f938",
       "tabbable": null,
       "tooltip": null,
       "value": 11487
      }
     },
     "41451c48bdd84d8a90b085b8d3efb6f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b83a590d21b4eff8d2286ac7126f938": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7fba5f664153432899f5182145d55da5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a9f3cdb05e4c4d498c2fdbe480dbd6a8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_2ee9bad19f0249d58d69eae77819bf86",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡11.5k/11.5kâ€‡[00:00&lt;00:00,â€‡762kB/s]"
      }
     },
     "905bc9603f20456fbfd9f2a465b359ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa65ba7b146f4ee884e85bbdf957e090",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a4e0a6d147ff46f581e2bded8cae8207",
       "tabbable": null,
       "tooltip": null,
       "value": "theme_schema%400.0.8.json:â€‡100%"
      }
     },
     "9f5ab752c521431a9f645b676b07e49f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4e0a6d147ff46f581e2bded8cae8207": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a9f3cdb05e4c4d498c2fdbe480dbd6a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5e5666a32164b98acc6c516d70e94a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_905bc9603f20456fbfd9f2a465b359ab",
        "IPY_MODEL_3a323e3fe9db452b9d3d7c042588b09b",
        "IPY_MODEL_7fba5f664153432899f5182145d55da5"
       ],
       "layout": "IPY_MODEL_41451c48bdd84d8a90b085b8d3efb6f8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fa65ba7b146f4ee884e85bbdf957e090": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
