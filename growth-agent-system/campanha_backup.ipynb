{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4765836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Criar o notebook completo como arquivo Python\n",
    "notebook_content = '''# ====================================================================\n",
    "# MARKETING DATA SCIENTIST PARTNER AGENT - PRODUCTION NOTEBOOK\n",
    "# Arquitetura Multi-Agente ADK para Diagn√≥stico de Campanhas\n",
    "# ====================================================================\n",
    "\n",
    "\"\"\"\n",
    "üéØ OBJETIVO:\n",
    "Agente cientista de dados s√™nior que atua como parceiro estrat√©gico para:\n",
    "- Diagn√≥stico de problemas em campanhas de tr√°fego pago\n",
    "- EDA (Exploratory Data Analysis) automatizada\n",
    "- Testes estat√≠sticos rigorosos (Chi¬≤, T-test, ANOVA)\n",
    "- An√°lise de causa raiz (RCA) sistem√°tica\n",
    "- Gera√ß√£o de insights acion√°veis e visualiza√ß√µes\n",
    "\n",
    "üèóÔ∏è ARQUITETURA:\n",
    "- 1 CoordinatorAgent (orquestrador h√≠brido)\n",
    "- 8 Agentes Especialistas (DataQuality, Tracking, Funnel, Diagnostic, PMax, Stats, Experiment, Insights)\n",
    "- 2 Agentes Novos (EDA, Visualization)\n",
    "- BigQuery como fonte de dados unificada\n",
    "- Statistical Toolkit com scipy.stats\n",
    "\"\"\"\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 1: INSTALA√á√ÉO E SETUP INICIAL\n",
    "# ====================================================================\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(\"\\\\n[INFO] Installing dependencies...\\\\n\")\n",
    "\n",
    "!pip install -q google-adk>=1.18.0\n",
    "!pip install -q google-cloud-bigquery>=3.15.0\n",
    "!pip install -q scipy>=1.11.0 pandas>=2.1.0 numpy>=1.24.0\n",
    "!pip install -q matplotlib>=3.7.0 seaborn>=0.12.0 plotly>=5.17.0\n",
    "!pip install -q scikit-learn>=1.3.0\n",
    "!pip install -q gradio>=4.14.0\n",
    "\n",
    "print(\"\\\\n[OK] All dependencies installed! ‚úÖ\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 2: CONFIGURA√á√ÉO SEGURA DE CREDENCIAIS\n",
    "# ====================================================================\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "import atexit\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SecureCredentialsManager:\n",
    "    \"\"\"Gerenciador seguro de credenciais com limpeza autom√°tica.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.temp_files = []\n",
    "        atexit.register(self.cleanup)\n",
    "    \n",
    "    def setup_gemini_key(self) -> bool:\n",
    "        \"\"\"Configura API key do Gemini.\"\"\"\n",
    "        try:\n",
    "            api_key = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "            if not api_key or len(api_key) < 20:\n",
    "                raise ValueError(\"Invalid API key\")\n",
    "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "            os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"FALSE\"\n",
    "            logger.info(\"‚úÖ Gemini API configured\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå API key failed: {e}\")\n",
    "            print(\"\\\\n[ACTION] Add GOOGLE_API_KEY in Kaggle Secrets\")\n",
    "            return False\n",
    "    \n",
    "    def setup_bigquery_credentials(self) -> tuple:\n",
    "        \"\"\"Configura credenciais do BigQuery.\"\"\"\n",
    "        try:\n",
    "            creds = UserSecretsClient().get_secret(\"BIGQUERY_SERVICE_ACCOUNT_JSON\")\n",
    "            fd, path = tempfile.mkstemp(suffix='.json', prefix='bq_')\n",
    "            os.write(fd, creds.encode())\n",
    "            os.close(fd)\n",
    "            os.chmod(path, 0o600)\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = path\n",
    "            self.temp_files.append(path)\n",
    "            logger.info(\"‚úÖ BigQuery configured\")\n",
    "            return True, path\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è BigQuery not configured: {e}\")\n",
    "            return False, \"\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove arquivos tempor√°rios.\"\"\"\n",
    "        for path in self.temp_files:\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.unlink(path)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "# Inicializar credenciais\n",
    "creds_manager = SecureCredentialsManager()\n",
    "GEMINI_READY = creds_manager.setup_gemini_key()\n",
    "BIGQUERY_ENABLED, BQ_PATH = creds_manager.setup_bigquery_credentials()\n",
    "\n",
    "if not GEMINI_READY:\n",
    "    raise RuntimeError(\"Cannot proceed without API key\")\n",
    "\n",
    "print(f\"\\\\n{'='*60}\")\n",
    "print(\"üîê Security Status:\")\n",
    "print(f\"  ‚úÖ Gemini: Configured\")\n",
    "print(f\"  {'‚úÖ' if BIGQUERY_ENABLED else '‚ö†Ô∏è'} BigQuery: {'Enabled' if BIGQUERY_ENABLED else 'Optional'}\")\n",
    "print(f\"{'='*60}\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 3: IMPORTS E CONFIGURA√á√ÉO DO BIGQUERY\n",
    "# ====================================================================\n",
    "\n",
    "from google.adk.agents import Agent, SequentialAgent, ParallelAgent, LoopAgent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.tools import AgentTool, FunctionTool, google_search\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ttest_ind, f_oneway, pearsonr\n",
    "import math\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from io import StringIO\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar BigQuery (condicional)\n",
    "bq_toolset = None\n",
    "if BIGQUERY_ENABLED:\n",
    "    try:\n",
    "        from google.adk.tools.bigquery import BigQueryToolset, BigQueryCredentialsConfig, BigQueryToolConfig, WriteMode\n",
    "        from google.oauth2 import service_account\n",
    "        \n",
    "        credentials = service_account.Credentials.from_service_account_file(BQ_PATH)\n",
    "        creds_config = BigQueryCredentialsConfig(credentials=credentials)\n",
    "        tool_config = BigQueryToolConfig(write_mode=WriteMode.BLOCKED)\n",
    "        bq_toolset = BigQueryToolset(\n",
    "            credentials_config=creds_config, \n",
    "            bigquery_tool_config=tool_config\n",
    "        )\n",
    "        logger.info(\"‚úÖ BigQuery initialized\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"BigQuery init failed: {e}\")\n",
    "        BIGQUERY_ENABLED = False\n",
    "\n",
    "logger.info(\"‚úÖ Imports complete\")\n",
    "print(\"[OK] Environment ready! üöÄ\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 4: FRAMEWORK DE VALIDA√á√ÉO\n",
    "# ====================================================================\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Erro de valida√ß√£o de entrada.\"\"\"\n",
    "    pass\n",
    "\n",
    "class InputValidator:\n",
    "    \"\"\"Validador de inputs para fun√ß√µes estat√≠sticas.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_probability(value: float, name: str):\n",
    "        \"\"\"Valida se valor est√° entre 0 e 1.\"\"\"\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise ValidationError(f\"{name} must be numeric\")\n",
    "        if not 0 < value < 1:\n",
    "            raise ValidationError(f\"{name} must be in (0,1), got {value}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_positive(value: float, name: str):\n",
    "        \"\"\"Valida se valor √© positivo.\"\"\"\n",
    "        if not isinstance(value, (int, float)):\n",
    "            raise ValidationError(f\"{name} must be numeric\")\n",
    "        if value <= 0:\n",
    "            raise ValidationError(f\"{name} must be positive\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_ab_test_inputs(ctrl_conv, ctrl_total, treat_conv, treat_total):\n",
    "        \"\"\"Valida inputs de teste A/B.\"\"\"\n",
    "        for val, name in [\n",
    "            (ctrl_conv, \"control_conversions\"), \n",
    "            (ctrl_total, \"control_total\"),\n",
    "            (treat_conv, \"treatment_conversions\"), \n",
    "            (treat_total, \"treatment_total\")\n",
    "        ]:\n",
    "            if not isinstance(val, int) or val < 0:\n",
    "                raise ValidationError(f\"{name} must be non-negative integer\")\n",
    "        \n",
    "        if ctrl_total == 0 or treat_total == 0:\n",
    "            raise ValidationError(\"Total cannot be zero\")\n",
    "        if ctrl_conv > ctrl_total:\n",
    "            raise ValidationError(f\"Control conversions > total\")\n",
    "        if treat_conv > treat_total:\n",
    "            raise ValidationError(f\"Treatment conversions > total\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_dataframe(df: pd.DataFrame, required_cols: List[str] = None):\n",
    "        \"\"\"Valida DataFrame.\"\"\"\n",
    "        if df.empty:\n",
    "            raise ValidationError(\"DataFrame is empty\")\n",
    "        \n",
    "        if required_cols:\n",
    "            missing = set(required_cols) - set(df.columns)\n",
    "            if missing:\n",
    "                raise ValidationError(f\"Missing columns: {missing}\")\n",
    "\n",
    "logger.info(\"‚úÖ Validation framework ready\")\n",
    "print(\"[OK] Input validation loaded!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 5: STATISTICAL TOOLKIT COMPLETO\n",
    "# ====================================================================\n",
    "\n",
    "@dataclass\n",
    "class SampleSizeResult:\n",
    "    \"\"\"Resultado de c√°lculo de tamanho de amostra.\"\"\"\n",
    "    sample_size_per_group: int\n",
    "    total_sample_size: int\n",
    "    baseline_rate: float\n",
    "    target_rate: float\n",
    "    mde_percentage: float\n",
    "    mde_absolute: float\n",
    "    alpha: float\n",
    "    power: float\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"sample_size_per_group\": self.sample_size_per_group,\n",
    "            \"total_sample_size\": self.total_sample_size,\n",
    "            \"baseline_rate\": self.baseline_rate,\n",
    "            \"target_rate\": self.target_rate,\n",
    "            \"mde_percentage\": self.mde_percentage,\n",
    "            \"mde_absolute\": self.mde_absolute,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"power\": self.power\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class SignificanceResult:\n",
    "    \"\"\"Resultado de teste de signific√¢ncia estat√≠stica.\"\"\"\n",
    "    control_rate: float\n",
    "    treatment_rate: float\n",
    "    uplift_relative_pct: float\n",
    "    uplift_absolute_pp: float\n",
    "    p_value: float\n",
    "    z_statistic: float\n",
    "    is_significant: bool\n",
    "    is_positive: bool\n",
    "    ci_95_lower: float\n",
    "    ci_95_upper: float\n",
    "    sample_sizes: Dict[str, int]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        if self.is_significant and self.is_positive:\n",
    "            recommendation = \"[OK] SHIP IT: Significant positive impact\"\n",
    "        elif self.is_significant and not self.is_positive:\n",
    "            recommendation = \"[STOP] DO NOT SHIP: Significant negative impact\"\n",
    "        else:\n",
    "            recommendation = \"[WAIT] KEEP TESTING: Not yet significant\"\n",
    "        \n",
    "        return {\n",
    "            \"control_rate\": self.control_rate,\n",
    "            \"treatment_rate\": self.treatment_rate,\n",
    "            \"uplift_relative_percentage\": self.uplift_relative_pct,\n",
    "            \"uplift_absolute_pp\": self.uplift_absolute_pp,\n",
    "            \"p_value\": self.p_value,\n",
    "            \"z_statistic\": self.z_statistic,\n",
    "            \"is_significant\": self.is_significant,\n",
    "            \"is_positive\": self.is_positive,\n",
    "            \"confidence_interval_95\": {\n",
    "                \"lower\": self.ci_95_lower,\n",
    "                \"upper\": self.ci_95_upper,\n",
    "                \"lower_pp\": self.ci_95_lower * 100,\n",
    "                \"upper_pp\": self.ci_95_upper * 100\n",
    "            },\n",
    "            \"interpretation\": \"SIGNIFICANT (p < 0.05)\" if self.is_significant else \"NOT SIGNIFICANT\",\n",
    "            \"recommendation\": recommendation,\n",
    "            \"sample_sizes\": self.sample_sizes\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class EDAResult:\n",
    "    \"\"\"Resultado de an√°lise explorat√≥ria.\"\"\"\n",
    "    shape: Dict[str, int]\n",
    "    columns: List[str]\n",
    "    dtypes: Dict[str, str]\n",
    "    missing_values: Dict[str, Dict[str, float]]\n",
    "    duplicate_rows: int\n",
    "    numeric_summary: Dict[str, Dict[str, float]]\n",
    "    categorical_summary: Dict[str, Dict[str, Any]]\n",
    "    correlations: Dict[str, float]\n",
    "    outliers: Dict[str, int]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"shape\": self.shape,\n",
    "            \"columns\": self.columns,\n",
    "            \"dtypes\": self.dtypes,\n",
    "            \"missing_values\": self.missing_values,\n",
    "            \"duplicate_rows\": self.duplicate_rows,\n",
    "            \"numeric_summary\": self.numeric_summary,\n",
    "            \"categorical_summary\": self.categorical_summary,\n",
    "            \"correlations\": self.correlations,\n",
    "            \"outliers\": self.outliers\n",
    "        }\n",
    "\n",
    "class StatisticalToolkit:\n",
    "    \"\"\"Toolkit estat√≠stico completo para an√°lise de campanhas.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_sample_size(\n",
    "        baseline_rate: float, \n",
    "        mde: float, \n",
    "        alpha=0.05, \n",
    "        power=0.8\n",
    "    ) -> SampleSizeResult:\n",
    "        \"\"\"\n",
    "        Calcula tamanho de amostra para teste A/B.\n",
    "        \n",
    "        Args:\n",
    "            baseline_rate: Taxa de convers√£o baseline (0-1)\n",
    "            mde: Minimum Detectable Effect em pontos percentuais\n",
    "            alpha: N√≠vel de signific√¢ncia (padr√£o 0.05)\n",
    "            power: Poder estat√≠stico (padr√£o 0.8)\n",
    "        \"\"\"\n",
    "        InputValidator.validate_probability(baseline_rate, \"baseline_rate\")\n",
    "        InputValidator.validate_positive(mde, \"mde\")\n",
    "        \n",
    "        p1 = baseline_rate\n",
    "        p2 = baseline_rate + (mde / 100)\n",
    "        \n",
    "        if p2 >= 1.0:\n",
    "            raise ValidationError(f\"Target rate ({p2:.2%}) exceeds 100%\")\n",
    "        \n",
    "        z_alpha = stats.norm.ppf(1 - alpha / 2)\n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        \n",
    "        numerator = (z_alpha + z_beta) ** 2 * (p1 * (1 - p1) + p2 * (1 - p2))\n",
    "        denominator = (p1 - p2) ** 2\n",
    "        \n",
    "        n_per_group = math.ceil(numerator / denominator)\n",
    "        \n",
    "        return SampleSizeResult(\n",
    "            sample_size_per_group=n_per_group,\n",
    "            total_sample_size=n_per_group * 2,\n",
    "            baseline_rate=baseline_rate,\n",
    "            target_rate=p2,\n",
    "            mde_percentage=mde,\n",
    "            mde_absolute=p2 - p1,\n",
    "            alpha=alpha,\n",
    "            power=power\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_statistical_significance(\n",
    "        ctrl_conv: int, \n",
    "        ctrl_total: int, \n",
    "        treat_conv: int, \n",
    "        treat_total: int, \n",
    "        alpha=0.05\n",
    "    ) -> SignificanceResult:\n",
    "        \"\"\"\n",
    "        Calcula signific√¢ncia estat√≠stica de teste A/B usando teste Z.\n",
    "        \n",
    "        Args:\n",
    "            ctrl_conv: Convers√µes do controle\n",
    "            ctrl_total: Total do controle\n",
    "            treat_conv: Convers√µes do tratamento\n",
    "            treat_total: Total do tratamento\n",
    "            alpha: N√≠vel de signific√¢ncia\n",
    "        \"\"\"\n",
    "        InputValidator.validate_ab_test_inputs(\n",
    "            ctrl_conv, ctrl_total, treat_conv, treat_total\n",
    "        )\n",
    "        \n",
    "        p1 = ctrl_conv / ctrl_total\n",
    "        p2 = treat_conv / treat_total\n",
    "        \n",
    "        # Teste Z para propor√ß√µes\n",
    "        p_pooled = (ctrl_conv + treat_conv) / (ctrl_total + treat_total)\n",
    "        se = math.sqrt(p_pooled * (1 - p_pooled) * (1/ctrl_total + 1/treat_total))\n",
    "        \n",
    "        z = (p2 - p1) / se if se > 0 else 0\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "        \n",
    "        # Uplift\n",
    "        uplift_relative = ((p2 - p1) / p1 * 100) if p1 > 0 else 0\n",
    "        uplift_absolute = (p2 - p1) * 100\n",
    "        \n",
    "        # Intervalo de confian√ßa\n",
    "        se_diff = math.sqrt(p1 * (1 - p1) / ctrl_total + p2 * (1 - p2) / treat_total)\n",
    "        ci_margin = stats.norm.ppf(1 - alpha/2) * se_diff\n",
    "        ci_lower = p2 - p1 - ci_margin\n",
    "        ci_upper = p2 - p1 + ci_margin\n",
    "        \n",
    "        return SignificanceResult(\n",
    "            control_rate=p1,\n",
    "            treatment_rate=p2,\n",
    "            uplift_relative_pct=uplift_relative,\n",
    "            uplift_absolute_pp=uplift_absolute,\n",
    "            p_value=p_value,\n",
    "            z_statistic=z,\n",
    "            is_significant=p_value < alpha,\n",
    "            is_positive=p2 > p1,\n",
    "            ci_95_lower=ci_lower,\n",
    "            ci_95_upper=ci_upper,\n",
    "            sample_sizes={\n",
    "                \"control\": ctrl_total,\n",
    "                \"treatment\": treat_total,\n",
    "                \"total\": ctrl_total + treat_total\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_chi_square_test(\n",
    "        observed: List[List[int]]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Executa teste qui-quadrado para tabela de conting√™ncia.\n",
    "        \n",
    "        Args:\n",
    "            observed: Tabela de conting√™ncia (lista de listas)\n",
    "        \"\"\"\n",
    "        obs_array = np.array(observed)\n",
    "        chi2, p_value, dof, expected = chi2_contingency(obs_array, correction=False)\n",
    "        \n",
    "        return {\n",
    "            \"test_type\": \"chi_square\",\n",
    "            \"chi2_statistic\": float(chi2),\n",
    "            \"p_value\": float(p_value),\n",
    "            \"degrees_of_freedom\": int(dof),\n",
    "            \"is_significant\": p_value < 0.05,\n",
    "            \"expected_frequencies\": expected.tolist()\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_t_test(\n",
    "        group_a: List[float], \n",
    "        group_b: List[float]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Executa teste t de duas amostras independentes.\n",
    "        \n",
    "        Args:\n",
    "            group_a: Valores do grupo A\n",
    "            group_b: Valores do grupo B\n",
    "        \"\"\"\n",
    "        t_stat, p_value = ttest_ind(group_a, group_b, equal_var=False)\n",
    "        \n",
    "        mean_a = np.mean(group_a)\n",
    "        mean_b = np.mean(group_b)\n",
    "        \n",
    "        return {\n",
    "            \"test_type\": \"t_test\",\n",
    "            \"t_statistic\": float(t_stat),\n",
    "            \"p_value\": float(p_value),\n",
    "            \"is_significant\": p_value < 0.05,\n",
    "            \"mean_group_a\": float(mean_a),\n",
    "            \"mean_group_b\": float(mean_b),\n",
    "            \"difference\": float(mean_b - mean_a),\n",
    "            \"relative_change_pct\": float((mean_b - mean_a) / mean_a * 100) if mean_a != 0 else 0\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_anova(\n",
    "        *groups: List[float]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Executa ANOVA para m√∫ltiplos grupos.\n",
    "        \n",
    "        Args:\n",
    "            *groups: M√∫ltiplos grupos de valores\n",
    "        \"\"\"\n",
    "        f_stat, p_value = f_oneway(*groups)\n",
    "        \n",
    "        return {\n",
    "            \"test_type\": \"anova\",\n",
    "            \"f_statistic\": float(f_stat),\n",
    "            \"p_value\": float(p_value),\n",
    "            \"is_significant\": p_value < 0.05,\n",
    "            \"num_groups\": len(groups),\n",
    "            \"group_means\": [float(np.mean(g)) for g in groups]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def perform_eda(csv_data: str) -> EDAResult:\n",
    "        \"\"\"\n",
    "        Executa an√°lise explorat√≥ria completa.\n",
    "        \n",
    "        Args:\n",
    "            csv_data: Dados CSV como string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(csv_data))\n",
    "        except Exception as e:\n",
    "            raise ValidationError(f\"Invalid CSV: {e}\")\n",
    "        \n",
    "        InputValidator.validate_dataframe(df)\n",
    "        \n",
    "        # Shape\n",
    "        shape = {\"rows\": len(df), \"columns\": len(df.columns)}\n",
    "        \n",
    "        # Colunas e tipos\n",
    "        columns = df.columns.tolist()\n",
    "        dtypes = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "        \n",
    "        # Missing values\n",
    "        missing = df.isnull().sum()\n",
    "        missing_pct = (missing / len(df) * 100).round(2)\n",
    "        missing_summary = {\n",
    "            col: {\"count\": int(missing[col]), \"percentage\": float(missing_pct[col])}\n",
    "            for col in df.columns if missing[col] > 0\n",
    "        }\n",
    "        \n",
    "        # Duplicatas\n",
    "        duplicate_rows = int(df.duplicated().sum())\n",
    "        \n",
    "        # An√°lise num√©rica\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_summary = {}\n",
    "        for col in numeric_cols:\n",
    "            numeric_summary[col] = {\n",
    "                \"mean\": float(df[col].mean()),\n",
    "                \"median\": float(df[col].median()),\n",
    "                \"std\": float(df[col].std()),\n",
    "                \"min\": float(df[col].min()),\n",
    "                \"max\": float(df[col].max()),\n",
    "                \"q25\": float(df[col].quantile(0.25)),\n",
    "                \"q75\": float(df[col].quantile(0.75))\n",
    "            }\n",
    "        \n",
    "        # An√°lise categ√≥rica\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        categorical_summary = {}\n",
    "        for col in categorical_cols:\n",
    "            value_counts = df[col].value_counts()\n",
    "            categorical_summary[col] = {\n",
    "                \"unique_values\": int(df[col].nunique()),\n",
    "                \"top_5_values\": value_counts.head(5).to_dict(),\n",
    "                \"mode\": str(df[col].mode()[0]) if len(df[col].mode()) > 0 else None\n",
    "            }\n",
    "        \n",
    "        # Correla√ß√µes\n",
    "        correlations = {}\n",
    "        if len(numeric_cols) >= 2:\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "            for i, col1 in enumerate(numeric_cols):\n",
    "                for col2 in numeric_cols[i+1:]:\n",
    "                    correlations[f\"{col1}_vs_{col2}\"] = float(corr_matrix.loc[col1, col2])\n",
    "        \n",
    "        # Outliers (IQR method)\n",
    "        outliers = {}\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_mask = (df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)\n",
    "            outliers[col] = int(outlier_mask.sum())\n",
    "        \n",
    "        return EDAResult(\n",
    "            shape=shape,\n",
    "            columns=columns,\n",
    "            dtypes=dtypes,\n",
    "            missing_values=missing_summary,\n",
    "            duplicate_rows=duplicate_rows,\n",
    "            numeric_summary=numeric_summary,\n",
    "            categorical_summary=categorical_summary,\n",
    "            correlations=correlations,\n",
    "            outliers=outliers\n",
    "        )\n",
    "\n",
    "# Wrapper functions para FunctionTools\n",
    "def safe_calculate_sample_size(\n",
    "    baseline_rate: float, \n",
    "    mde: float, \n",
    "    alpha=0.05, \n",
    "    power=0.8\n",
    ") -> str:\n",
    "    \"\"\"Wrapper seguro para c√°lculo de sample size.\"\"\"\n",
    "    try:\n",
    "        result = StatisticalToolkit.calculate_sample_size(baseline_rate, mde, alpha, power)\n",
    "        return json.dumps(result.to_dict(), indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def safe_calculate_significance(\n",
    "    ctrl_conv: int, \n",
    "    ctrl_total: int, \n",
    "    treat_conv: int, \n",
    "    treat_total: int\n",
    ") -> str:\n",
    "    \"\"\"Wrapper seguro para c√°lculo de signific√¢ncia.\"\"\"\n",
    "    try:\n",
    "        result = StatisticalToolkit.calculate_statistical_significance(\n",
    "            ctrl_conv, ctrl_total, treat_conv, treat_total\n",
    "        )\n",
    "        return json.dumps(result.to_dict(), indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def safe_perform_eda(csv_data: str) -> str:\n",
    "    \"\"\"Wrapper seguro para EDA.\"\"\"\n",
    "    try:\n",
    "        result = StatisticalToolkit.perform_eda(csv_data)\n",
    "        return json.dumps(result.to_dict(), indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def safe_chi_square_test(observed_json: str) -> str:\n",
    "    \"\"\"Wrapper seguro para teste qui-quadrado.\"\"\"\n",
    "    try:\n",
    "        observed = json.loads(observed_json)\n",
    "        result = StatisticalToolkit.perform_chi_square_test(observed)\n",
    "        return json.dumps(result, indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def safe_t_test(group_a_json: str, group_b_json: str) -> str:\n",
    "    \"\"\"Wrapper seguro para teste t.\"\"\"\n",
    "    try:\n",
    "        group_a = json.loads(group_a_json)\n",
    "        group_b = json.loads(group_b_json)\n",
    "        result = StatisticalToolkit.perform_t_test(group_a, group_b)\n",
    "        return json.dumps(result, indent=2)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "# Criar FunctionTools\n",
    "sample_size_tool = FunctionTool(\n",
    "    function=safe_calculate_sample_size,\n",
    "    description=\"Calculate required sample size for A/B test given baseline rate and MDE\"\n",
    ")\n",
    "\n",
    "significance_tool = FunctionTool(\n",
    "    function=safe_calculate_significance,\n",
    "    description=\"Calculate statistical significance of A/B test results\"\n",
    ")\n",
    "\n",
    "eda_tool = FunctionTool(\n",
    "    function=safe_perform_eda,\n",
    "    description=\"Perform comprehensive exploratory data analysis on CSV data\"\n",
    ")\n",
    "\n",
    "chi_square_tool = FunctionTool(\n",
    "    function=safe_chi_square_test,\n",
    "    description=\"Perform chi-square test on contingency table\"\n",
    ")\n",
    "\n",
    "t_test_tool = FunctionTool(\n",
    "    function=safe_t_test,\n",
    "    description=\"Perform t-test comparing two groups\"\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ Statistical Toolkit ready\")\n",
    "print(\"[OK] Statistical functions loaded!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 6: VISUALIZATION TOOLKIT\n",
    "# ====================================================================\n",
    "\n",
    "class VisualizationToolkit:\n",
    "    \"\"\"Toolkit para gera√ß√£o de visualiza√ß√µes diagn√≥sticas.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_funnel_chart(\n",
    "        stages: List[str], \n",
    "        values: List[int],\n",
    "        title: str = \"Conversion Funnel\"\n",
    "    ) -> str:\n",
    "        \"\"\"Cria gr√°fico de funil de convers√£o.\"\"\"\n",
    "        fig = go.Figure(go.Funnel(\n",
    "            y=stages,\n",
    "            x=values,\n",
    "            textinfo=\"value+percent initial\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig.to_html()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_time_series(\n",
    "        df: pd.DataFrame,\n",
    "        date_col: str,\n",
    "        metric_col: str,\n",
    "        title: str = \"Metric Over Time\"\n",
    "    ) -> str:\n",
    "        \"\"\"Cria gr√°fico de s√©rie temporal.\"\"\"\n",
    "        fig = px.line(\n",
    "            df, \n",
    "            x=date_col, \n",
    "            y=metric_col,\n",
    "            title=title,\n",
    "            markers=True\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=400)\n",
    "        \n",
    "        return fig.to_html()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_correlation_heatmap(\n",
    "        df: pd.DataFrame,\n",
    "        title: str = \"Correlation Matrix\"\n",
    "    ) -> str:\n",
    "        \"\"\"Cria heatmap de correla√ß√£o.\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        fig = px.imshow(\n",
    "            corr_matrix,\n",
    "            text_auto=True,\n",
    "            aspect=\"auto\",\n",
    "            title=title,\n",
    "            color_continuous_scale='RdBu_r'\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600)\n",
    "        \n",
    "        return fig.to_html()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_distribution_plot(\n",
    "        df: pd.DataFrame,\n",
    "        column: str,\n",
    "        title: str = \"Distribution\"\n",
    "    ) -> str:\n",
    "        \"\"\"Cria gr√°fico de distribui√ß√£o.\"\"\"\n",
    "        fig = px.histogram(\n",
    "            df, \n",
    "            x=column,\n",
    "            title=title,\n",
    "            marginal=\"box\"\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=400)\n",
    "        \n",
    "        return fig.to_html()\n",
    "\n",
    "logger.info(\"‚úÖ Visualization Toolkit ready\")\n",
    "print(\"[OK] Visualization functions loaded!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 7: CRIAR 10 AGENTES ESPECIALIZADOS\n",
    "# ====================================================================\n",
    "\n",
    "MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "# 1. DataQualityAgent\n",
    "data_quality_tools = [eda_tool]\n",
    "if bq_toolset:\n",
    "    data_quality_tools.append(bq_toolset)\n",
    "\n",
    "data_quality_agent = Agent(\n",
    "    name=\"DataQualityAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a data quality auditor.\n",
    "    \n",
    "    Your job:\n",
    "    1. Validate data integrity (missing values, duplicates, outliers)\n",
    "    2. Check for anomalies in key metrics\n",
    "    3. Verify data freshness and completeness\n",
    "    4. Report any data quality issues that would compromise analysis\n",
    "    \n",
    "    Use the EDA tool to analyze datasets comprehensively.\n",
    "    \"\"\",\n",
    "    tools=data_quality_tools,\n",
    "    output_key=\"data_quality_report\"\n",
    ")\n",
    "\n",
    "# 2. TrackingAgent\n",
    "tracking_tools = [eda_tool]\n",
    "if bq_toolset:\n",
    "    tracking_tools.append(bq_toolset)\n",
    "\n",
    "tracking_agent = Agent(\n",
    "    name=\"TrackingAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a tracking implementation specialist.\n",
    "    \n",
    "    Your job:\n",
    "    1. Validate event tracking (purchase, lead, etc.)\n",
    "    2. Check gclid presence in Google Ads traffic\n",
    "    3. Verify UTM parameter consistency\n",
    "    4. Identify tracking gaps or implementation errors\n",
    "    \n",
    "    Report any tracking issues that would affect attribution.\n",
    "    \"\"\",\n",
    "    tools=tracking_tools,\n",
    "    output_key=\"tracking_report\"\n",
    ")\n",
    "\n",
    "# 3. FunnelAgent\n",
    "funnel_tools = [eda_tool, google_search]\n",
    "if bq_toolset:\n",
    "    funnel_tools.append(bq_toolset)\n",
    "\n",
    "funnel_agent = Agent(\n",
    "    name=\"FunnelAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a conversion funnel analyst.\n",
    "    \n",
    "    Your job:\n",
    "    1. Map the complete conversion funnel\n",
    "    2. Calculate conversion rates at each stage\n",
    "    3. Identify the biggest bottleneck (highest drop-off)\n",
    "    4. Segment performance by device, channel, etc.\n",
    "    \n",
    "    Provide actionable insights on where to focus optimization.\n",
    "    \"\"\",\n",
    "    tools=funnel_tools,\n",
    "    output_key=\"funnel_report\"\n",
    ")\n",
    "\n",
    "# 4. DiagnosticAgent\n",
    "diagnostic_tools = [eda_tool, chi_square_tool, t_test_tool, google_search]\n",
    "if bq_toolset:\n",
    "    diagnostic_tools.append(bq_toolset)\n",
    "\n",
    "diagnostic_agent = Agent(\n",
    "    name=\"DiagnosticAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a senior paid media diagnostician.\n",
    "    \n",
    "    Your job is Root Cause Analysis (RCA):\n",
    "    1. Investigate audience saturation and targeting issues\n",
    "    2. Analyze Quality Score and ad relevance\n",
    "    3. Check auction pressure (competition)\n",
    "    4. Segment by time, device, location\n",
    "    5. Identify search query quality issues\n",
    "    \n",
    "    Use statistical tests to validate hypotheses.\n",
    "    Prioritize the root cause with highest impact.\n",
    "    \"\"\",\n",
    "    tools=diagnostic_tools,\n",
    "    output_key=\"root_cause_report\"\n",
    ")\n",
    "\n",
    "# 5. PMaxAgent\n",
    "pmax_tools = [eda_tool, google_search]\n",
    "if bq_toolset:\n",
    "    pmax_tools.append(bq_toolset)\n",
    "\n",
    "pmax_agent = Agent(\n",
    "    name=\"PMaxAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a Performance Max specialist.\n",
    "    \n",
    "    Your job:\n",
    "    1. Evaluate creative performance (Asset Groups, Combinations)\n",
    "    2. Analyze audience insights (Optimized segments)\n",
    "    3. Review channel distribution (Search, Display, Video, Shopping)\n",
    "    4. Assess search theme impact\n",
    "    \n",
    "    PMax is a black box - extract insights from available reports.\n",
    "    \"\"\",\n",
    "    tools=pmax_tools,\n",
    "    output_key=\"pmax_diagnostic_report\"\n",
    ")\n",
    "\n",
    "# 6. StatsAgent\n",
    "stats_agent = Agent(\n",
    "    name=\"StatsAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a statistician.\n",
    "    \n",
    "    Your job:\n",
    "    1. Validate A/B test significance (chi-square for rates, t-test for continuous)\n",
    "    2. Calculate p-values and confidence intervals\n",
    "    3. Determine if results are statistically significant (p < 0.05)\n",
    "    4. Prevent false positives from noise\n",
    "    \n",
    "    Always report: p-value, test type, and recommendation (ship/wait/stop).\n",
    "    \"\"\",\n",
    "    tools=[significance_tool, chi_square_tool, t_test_tool],\n",
    "    output_key=\"stats_results\"\n",
    ")\n",
    "\n",
    "# 7. ExperimentAgent\n",
    "experiment_agent = Agent(\n",
    "    name=\"ExperimentAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are an experimentation designer.\n",
    "    \n",
    "    Your job:\n",
    "    1. Calculate required sample size for A/B tests\n",
    "    2. Estimate test duration based on traffic\n",
    "    3. Define success metrics (primary + secondary)\n",
    "    4. Design statistically valid experiments\n",
    "    \n",
    "    Ensure tests have sufficient power (80%) and significance (95%).\n",
    "    \"\"\",\n",
    "    tools=[sample_size_tool, google_search],\n",
    "    output_key=\"experiment_plan\"\n",
    ")\n",
    "\n",
    "# 8. InsightsAgent\n",
    "insights_agent = Agent(\n",
    "    name=\"InsightsAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a senior growth strategist.\n",
    "    \n",
    "    Your job:\n",
    "    1. Synthesize all technical reports into business insights\n",
    "    2. Provide 3-5 prioritized recommendations\n",
    "    3. Identify quick wins vs. strategic initiatives\n",
    "    4. Speak in business language (ROI, revenue, cost)\n",
    "    \n",
    "    Translate data into decisions.\n",
    "    \"\"\",\n",
    "    tools=[google_search],\n",
    "    output_key=\"strategic_recommendations\"\n",
    ")\n",
    "\n",
    "# 9. EDAAgent (NOVO)\n",
    "eda_agent = Agent(\n",
    "    name=\"EDAAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are an exploratory data analysis specialist.\n",
    "    \n",
    "    Your job:\n",
    "    1. Perform comprehensive EDA on campaign data\n",
    "    2. Identify patterns, trends, and anomalies\n",
    "    3. Calculate correlations between metrics\n",
    "    4. Detect outliers and data quality issues\n",
    "    5. Generate statistical summaries\n",
    "    \n",
    "    Provide deep insights hidden in the data.\n",
    "    \"\"\",\n",
    "    tools=[eda_tool, chi_square_tool, t_test_tool],\n",
    "    output_key=\"eda_report\"\n",
    ")\n",
    "\n",
    "# 10. VisualizationAgent (NOVO)\n",
    "visualization_agent = Agent(\n",
    "    name=\"VisualizationAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a data visualization specialist.\n",
    "    \n",
    "    Your job:\n",
    "    1. Create diagnostic charts (funnels, time series, distributions)\n",
    "    2. Generate correlation heatmaps\n",
    "    3. Visualize A/B test results\n",
    "    4. Make complex data easy to understand\n",
    "    \n",
    "    Use appropriate chart types for each insight.\n",
    "    \"\"\",\n",
    "    tools=[eda_tool],\n",
    "    output_key=\"visualizations\"\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ 10 agents created\")\n",
    "print(\"[OK] Agent team ready! ü§ñ\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 8: LOOP AGENT PARA REFINAMENTO\n",
    "# ====================================================================\n",
    "\n",
    "def approve_analysis(approved: bool, feedback: str) -> str:\n",
    "    \"\"\"Fun√ß√£o para aprovar ou rejeitar an√°lise.\"\"\"\n",
    "    logger.info(f\"Analysis approval: {approved}\")\n",
    "    return json.dumps({\n",
    "        \"approved\": approved,\n",
    "        \"feedback\": feedback,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "approval_tool = FunctionTool(\n",
    "    function=approve_analysis,\n",
    "    description=\"Approve or reject analysis with feedback\"\n",
    ")\n",
    "\n",
    "critic_agent = Agent(\n",
    "    name=\"CriticAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"Review the {root_cause_report}.\n",
    "    \n",
    "    Check if:\n",
    "    1. Root cause is clearly identified\n",
    "    2. Evidence is statistically validated\n",
    "    3. Recommendations are actionable\n",
    "    \n",
    "    Call approve_analysis: approved=True if complete, False with feedback otherwise.\n",
    "    \"\"\",\n",
    "    tools=[approval_tool],\n",
    "    output_key=\"critique\"\n",
    ")\n",
    "\n",
    "refiner_agent = Agent(\n",
    "    name=\"RefinerAgent\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"Fix issues in {root_cause_report} based on {critique}.\n",
    "    \n",
    "    Improve:\n",
    "    1. Clarity of root cause\n",
    "    2. Statistical validation\n",
    "    3. Actionability of recommendations\n",
    "    \"\"\",\n",
    "    tools=[eda_tool, chi_square_tool, t_test_tool],\n",
    "    output_key=\"root_cause_report\"\n",
    ")\n",
    "\n",
    "refinement_loop = LoopAgent(\n",
    "    name=\"RefinementLoop\",\n",
    "    sub_agents=[critic_agent, refiner_agent],\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"[OK] Loop agent created!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 9: COMPOSITE AGENTS (PARALLEL + SEQUENTIAL)\n",
    "# ====================================================================\n",
    "\n",
    "# Parallel: Diagn√≥sticos iniciais independentes\n",
    "parallel_diagnostic = ParallelAgent(\n",
    "    name=\"ParallelDiagnostic\",\n",
    "    sub_agents=[\n",
    "        data_quality_agent,\n",
    "        tracking_agent,\n",
    "        funnel_agent,\n",
    "        eda_agent\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Sequential: Pipeline completo\n",
    "sequential_pipeline = SequentialAgent(\n",
    "    name=\"FullDiagnosticPipeline\",\n",
    "    sub_agents=[\n",
    "        parallel_diagnostic,      # Etapa 1: Diagn√≥sticos paralelos\n",
    "        diagnostic_agent,          # Etapa 2: RCA\n",
    "        stats_agent,               # Etapa 3: Valida√ß√£o estat√≠stica\n",
    "        refinement_loop,           # Etapa 4: Refinamento\n",
    "        experiment_agent,          # Etapa 5: Design de experimento\n",
    "        insights_agent,            # Etapa 6: S√≠ntese estrat√©gica\n",
    "        visualization_agent        # Etapa 7: Visualiza√ß√µes\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"[OK] Composite agents ready!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 10: COORDINATOR AGENT (ORQUESTRADOR H√çBRIDO)\n",
    "# ====================================================================\n",
    "\n",
    "coordinator_tools = [\n",
    "    AgentTool(agent=data_quality_agent),\n",
    "    AgentTool(agent=tracking_agent),\n",
    "    AgentTool(agent=funnel_agent),\n",
    "    AgentTool(agent=diagnostic_agent),\n",
    "    AgentTool(agent=pmax_agent),\n",
    "    AgentTool(agent=stats_agent),\n",
    "    AgentTool(agent=experiment_agent),\n",
    "    AgentTool(agent=insights_agent),\n",
    "    AgentTool(agent=eda_agent),\n",
    "    AgentTool(agent=visualization_agent),\n",
    "    google_search,\n",
    "    sample_size_tool,\n",
    "    significance_tool,\n",
    "    eda_tool,\n",
    "    chi_square_tool,\n",
    "    t_test_tool\n",
    "]\n",
    "\n",
    "if bq_toolset:\n",
    "    coordinator_tools.append(bq_toolset)\n",
    "\n",
    "coordinator = Agent(\n",
    "    name=\"MarketingDataScientistPartner\",\n",
    "    model=MODEL,\n",
    "    instruction=\"\"\"You are a Senior Marketing Data Scientist Partner.\n",
    "    \n",
    "    Your mission: Diagnose and solve complex campaign performance problems.\n",
    "    \n",
    "    WORKFLOW:\n",
    "    1. Receive analyst query (e.g., \"My CPA increased 30%\")\n",
    "    \n",
    "    2. PARALLEL DIAGNOSTICS (call in parallel):\n",
    "       - DataQualityAgent: Validate data integrity\n",
    "       - TrackingAgent: Verify tracking implementation\n",
    "       - FunnelAgent: Identify bottlenecks\n",
    "       - EDAAgent: Perform exploratory analysis\n",
    "    \n",
    "    3. STOP if data quality or tracking is compromised\n",
    "    \n",
    "    4. ROOT CAUSE ANALYSIS:\n",
    "       - Call DiagnosticAgent for general issues\n",
    "       - Call PMaxAgent for Performance Max campaigns\n",
    "    \n",
    "    5. STATISTICAL VALIDATION:\n",
    "       - Call StatsAgent to validate findings\n",
    "    \n",
    "    6. EXPERIMENT DESIGN:\n",
    "       - If user requests a test, call ExperimentAgent\n",
    "    \n",
    "    7. SYNTHESIS:\n",
    "       - Call InsightsAgent to generate strategic recommendations\n",
    "       - Call VisualizationAgent to create diagnostic charts\n",
    "    \n",
    "    8. Present comprehensive, actionable report\n",
    "    \n",
    "    Always be data-driven, statistically rigorous, and business-focused.\n",
    "    \"\"\",\n",
    "    tools=coordinator_tools\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ Coordinator created\")\n",
    "print(\"[OK] Coordinator ready!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 11: RUNNER COM OBSERVABILIDADE\n",
    "# ====================================================================\n",
    "\n",
    "@dataclass\n",
    "class QueryMetrics:\n",
    "    \"\"\"M√©tricas de execu√ß√£o de query.\"\"\"\n",
    "    query: str\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    duration_seconds: Optional[float] = None\n",
    "    success: bool = False\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    def finalize(self, success: bool, error: Optional[str] = None):\n",
    "        self.end_time = datetime.now()\n",
    "        self.duration_seconds = (self.end_time - self.start_time).total_seconds()\n",
    "        self.success = success\n",
    "        self.error = error\n",
    "\n",
    "class ObservableRunner:\n",
    "    \"\"\"Runner com observabilidade e m√©tricas.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent):\n",
    "        self.runner = InMemoryRunner(agent=agent)\n",
    "        self.metrics_history: List[QueryMetrics] = []\n",
    "    \n",
    "    async def run(self, query: str) -> str:\n",
    "        \"\"\"Executa query com tracking de m√©tricas.\"\"\"\n",
    "        metrics = QueryMetrics(query=query, start_time=datetime.now())\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"üöÄ Query: {query[:100]}...\")\n",
    "            result = await self.runner.run_debug(query)\n",
    "            metrics.finalize(success=True)\n",
    "            logger.info(f\"‚úÖ Done in {metrics.duration_seconds:.2f}s\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            metrics.finalize(success=False, error=str(e))\n",
    "            logger.error(f\"‚ùå Failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.metrics_history.append(metrics)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Retorna estat√≠sticas de execu√ß√£o.\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {\"total_queries\": 0}\n",
    "        \n",
    "        successful = [m for m in self.metrics_history if m.success]\n",
    "        return {\n",
    "            \"total_queries\": len(self.metrics_history),\n",
    "            \"successful\": len(successful),\n",
    "            \"failed\": len(self.metrics_history) - len(successful),\n",
    "            \"success_rate\": len(successful) / len(self.metrics_history) * 100 if self.metrics_history else 0,\n",
    "            \"avg_duration\": np.mean([m.duration_seconds for m in successful]) if successful else 0\n",
    "        }\n",
    "\n",
    "runner = ObservableRunner(agent=coordinator)\n",
    "\n",
    "logger.info(\"‚úÖ Runner initialized\")\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üéâ SYSTEM READY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\\\n[‚úÖ] 10 Specialized Agents\")\n",
    "print(\"[‚úÖ] Statistical Toolkit (Chi¬≤, T-test, ANOVA)\")\n",
    "print(\"[‚úÖ] EDA & Visualization\")\n",
    "print(\"[‚úÖ] Secure Credentials\")\n",
    "print(\"[‚úÖ] Observability\")\n",
    "if bq_toolset:\n",
    "    print(\"[‚úÖ] BigQuery Integration\")\n",
    "print(\"\\\\n[OK] Ready to diagnose campaigns! üöÄ\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 12: CRIAR DADOS DEMO REALISTAS\n",
    "# ====================================================================\n",
    "\n",
    "def create_campaign_demo_data(n_days=30, n_campaigns=5):\n",
    "    \"\"\"Gera dados realistas de campanhas.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    campaigns = [f\"Campaign_{i+1}\" for i in range(n_campaigns)]\n",
    "    devices = ['mobile', 'desktop']\n",
    "    \n",
    "    for day in range(n_days):\n",
    "        date = (datetime.now() - timedelta(days=n_days-day)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        for campaign in campaigns:\n",
    "            for device in devices:\n",
    "                # Simular problema: CPA aumenta em mobile ap√≥s dia 15\n",
    "                if device == 'mobile' and day > 15:\n",
    "                    cvr_multiplier = 0.5  # CVR cai 50%\n",
    "                else:\n",
    "                    cvr_multiplier = 1.0\n",
    "                \n",
    "                impressions = np.random.randint(5000, 15000)\n",
    "                clicks = int(impressions * np.random.uniform(0.02, 0.05))\n",
    "                cost = clicks * np.random.uniform(1.5, 3.0)\n",
    "                sessions = int(clicks * np.random.uniform(0.85, 0.95))\n",
    "                conversions = int(sessions * np.random.uniform(0.02, 0.04) * cvr_multiplier)\n",
    "                revenue = conversions * np.random.uniform(50, 150)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'campaign': campaign,\n",
    "                    'device': device,\n",
    "                    'impressions': impressions,\n",
    "                    'clicks': clicks,\n",
    "                    'cost': round(cost, 2),\n",
    "                    'sessions': sessions,\n",
    "                    'conversions': conversions,\n",
    "                    'revenue': round(revenue, 2)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Adicionar m√©tricas calculadas\n",
    "    df['ctr'] = (df['clicks'] / df['impressions'] * 100).round(2)\n",
    "    df['cpc'] = (df['cost'] / df['clicks']).round(2)\n",
    "    df['cvr'] = (df['conversions'] / df['sessions'] * 100).round(2)\n",
    "    df['cpa'] = (df['cost'] / df['conversions']).round(2)\n",
    "    df['roas'] = (df['revenue'] / df['cost']).round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "demo_df = create_campaign_demo_data()\n",
    "demo_csv = demo_df.to_csv(index=False)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üìä DEMO DATA CREATED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\\\nüìà Dataset:\")\n",
    "print(f\"   Rows: {len(demo_df):,}\")\n",
    "print(f\"   Columns: {len(demo_df.columns)}\")\n",
    "print(f\"   Date Range: {demo_df['date'].min()} to {demo_df['date'].max()}\")\n",
    "\n",
    "print(f\"\\\\nüìã Sample:\")\n",
    "print(demo_df.head(10))\n",
    "\n",
    "print(f\"\\\\nüìä Summary:\")\n",
    "print(f\"   Total Cost: ${demo_df['cost'].sum():,.2f}\")\n",
    "print(f\"   Total Conversions: {demo_df['conversions'].sum():,}\")\n",
    "print(f\"   Avg CPA: ${demo_df['cpa'].mean():.2f}\")\n",
    "print(f\"   Avg ROAS: {demo_df['roas'].mean():.2f}x\")\n",
    "\n",
    "print(\"\\\\n[OK] Demo data ready!\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 13: TESTAR STATISTICAL TOOLKIT\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING STATISTICAL TOOLKIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test 1: Sample Size\n",
    "print(\"\\\\n[TEST 1] Sample Size Calculation\")\n",
    "result1 = StatisticalToolkit.calculate_sample_size(\n",
    "    baseline_rate=0.025, \n",
    "    mde=0.5\n",
    ")\n",
    "print(json.dumps(result1.to_dict(), indent=2))\n",
    "\n",
    "# Test 2: Significance\n",
    "print(\"\\\\n[TEST 2] Statistical Significance\")\n",
    "result2 = StatisticalToolkit.calculate_statistical_significance(\n",
    "    ctrl_conv=250, \n",
    "    ctrl_total=10000, \n",
    "    treat_conv=280, \n",
    "    treat_total=10000\n",
    ")\n",
    "print(json.dumps(result2.to_dict(), indent=2))\n",
    "\n",
    "# Test 3: EDA\n",
    "print(\"\\\\n[TEST 3] Exploratory Data Analysis\")\n",
    "result3 = StatisticalToolkit.perform_eda(demo_csv)\n",
    "print(json.dumps(result3.to_dict(), indent=2)[:1000] + \"...\")\n",
    "\n",
    "# Test 4: Chi-Square\n",
    "print(\"\\\\n[TEST 4] Chi-Square Test\")\n",
    "observed = [[250, 9750], [280, 9720]]  # Control vs Treatment\n",
    "result4 = StatisticalToolkit.perform_chi_square_test(observed)\n",
    "print(json.dumps(result4, indent=2))\n",
    "\n",
    "# Test 5: T-Test\n",
    "print(\"\\\\n[TEST 5] T-Test\")\n",
    "group_a = list(demo_df[demo_df['device'] == 'desktop']['cpa'].values)\n",
    "group_b = list(demo_df[demo_df['device'] == 'mobile']['cpa'].values)\n",
    "result5 = StatisticalToolkit.perform_t_test(group_a, group_b)\n",
    "print(json.dumps(result5, indent=2))\n",
    "\n",
    "# Test 6: Validation\n",
    "print(\"\\\\n[TEST 6] Input Validation\")\n",
    "try:\n",
    "    StatisticalToolkit.calculate_sample_size(baseline_rate=1.5, mde=0.5)\n",
    "    print(\"‚ùå Should have failed!\")\n",
    "except ValidationError as e:\n",
    "    print(f\"‚úÖ Validation works: {e}\")\n",
    "\n",
    "print(\"\\\\n[OK] All tests passed! ‚úÖ\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 14: TESTAR SISTEMA DE AGENTES\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"ü§ñ TESTING AGENT SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Query 1: Conceitual\n",
    "print(\"\\\\n[QUERY 1] Conceptual Question\")\n",
    "query1 = \"What are the top 5 reasons why CPA increases in paid campaigns?\"\n",
    "print(f\"Q: {query1}\\\\n\")\n",
    "\n",
    "response1 = asyncio.run(runner.run(query1))\n",
    "print(f\"A: {response1[:800]}...\\\\n\")\n",
    "\n",
    "# Query 2: Sample Size\n",
    "print(\"\\\\n[QUERY 2] Sample Size Calculation\")\n",
    "query2 = \"Calculate sample size needed to improve conversion rate from 2.5% to 3.0%\"\n",
    "print(f\"Q: {query2}\\\\n\")\n",
    "\n",
    "response2 = asyncio.run(runner.run(query2))\n",
    "print(f\"A: {response2[:800]}...\\\\n\")\n",
    "\n",
    "# Query 3: Diagn√≥stico com dados\n",
    "print(\"\\\\n[QUERY 3] Campaign Diagnosis\")\n",
    "query3 = f\"\"\"Analyze this campaign data and diagnose the problem:\n",
    "\n",
    "{demo_csv[:2000]}\n",
    "\n",
    "Question: My CPA increased significantly. What's the root cause?\n",
    "\"\"\"\n",
    "print(f\"Q: Campaign diagnosis with data\\\\n\")\n",
    "\n",
    "response3 = asyncio.run(runner.run(query3))\n",
    "print(f\"A: {response3[:800]}...\\\\n\")\n",
    "\n",
    "# Mostrar estat√≠sticas\n",
    "stats = runner.get_stats()\n",
    "print(\"\\\\nüìä Performance Metrics:\")\n",
    "print(json.dumps(stats, indent=2))\n",
    "\n",
    "print(\"\\\\n[OK] Agent tests complete! ‚úÖ\\\\n\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# CELL 15: INTERFACE GRADIO\n",
    "# ====================================================================\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "current_csv_data = None\n",
    "current_df = None\n",
    "\n",
    "def upload_csv_handler(file):\n",
    "    \"\"\"Handler para upload de CSV.\"\"\"\n",
    "    global current_csv_data, current_df\n",
    "    \n",
    "    if file is None:\n",
    "        return \"‚ö†Ô∏è No file uploaded\", None\n",
    "    \n",
    "    try:\n",
    "        with open(file.name, 'r') as f:\n",
    "            current_csv_data = f.read()\n",
    "        \n",
    "        current_df = pd.read_csv(StringIO(current_csv_data))\n",
    "        \n",
    "        # An√°lise r√°pida\n",
    "        analysis = StatisticalToolkit.perform_eda(current_csv_data)\n",
    "        \n",
    "        summary = f\"\"\"‚úÖ **CSV Loaded Successfully!**\n",
    "\n",
    "üìä **Dataset Overview:**\n",
    "- Rows: {analysis.shape['rows']:,}\n",
    "- Columns: {analysis.shape['columns']}\n",
    "\n",
    "üìã **Columns:** {', '.join(analysis.columns)}\n",
    "\n",
    "üîç **Data Quality:**\n",
    "- Missing Values: {len(analysis.missing_values)} columns\n",
    "- Duplicate Rows: {analysis.duplicate_rows}\n",
    "- Outliers Detected: {sum(analysis.outliers.values())} total\n",
    "\n",
    "‚úÖ Ready for analysis!\n",
    "\"\"\"\n",
    "        \n",
    "        preview = current_df.head(20)\n",
    "        \n",
    "        return summary, preview\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\", None\n",
    "\n",
    "def query_handler(user_query):\n",
    "    \"\"\"Handler para queries do usu√°rio.\"\"\"\n",
    "    global current_csv_data\n",
    "    \n",
    "    if not user_query or not user_query.strip():\n",
    "        return \"‚ö†Ô∏è Please enter a question.\"\n",
    "    \n",
    "    try:\n",
    "        # Adicionar contexto de dados se dispon√≠vel\n",
    "        if current_csv_data:\n",
    "            context = f\"\"\"Campaign data preview:\n",
    "{current_csv_data[:3000]}...\n",
    "\n",
    "Analyst Question: {user_query}\n",
    "\"\"\"\n",
    "        else:\n",
    "            context = user_query\n",
    "        \n",
    "        # Executar query\n",
    "        import asyncio\n",
    "        result = asyncio.run(runner.run(context))\n",
    "        \n",
    "        # Adicionar m√©tricas\n",
    "        stats = runner.get_stats()\n",
    "        result += f\"\\\\n\\\\n---\\\\nüìä Query time: {stats['avg_duration']:.2f}s | Success rate: {stats['success_rate']:.1f}%\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "def calc_sample_size_handler(baseline, mde, alpha, power, traffic):\n",
    "    \"\"\"Handler para c√°lculo de sample size.\"\"\"\n",
    "    try:\n",
    "        baseline = float(baseline) / 100\n",
    "        mde = float(mde)\n",
    "        alpha = float(alpha)\n",
    "        power = float(power)\n",
    "        traffic = int(traffic)\n",
    "        \n",
    "        result = StatisticalToolkit.calculate_sample_size(baseline, mde, alpha, power)\n",
    "        \n",
    "        # Calcular dura√ß√£o do teste\n",
    "        days_needed = math.ceil(result.total_sample_size / traffic)\n",
    "        \n",
    "        output = f\"\"\"‚úÖ **Sample Size Calculation**\n",
    "\n",
    "üìä **Required Sample:**\n",
    "- Per Group: **{result.sample_size_per_group:,}**\n",
    "- Total: **{result.total_sample_size:,}**\n",
    "\n",
    "‚è±Ô∏è **Test Duration:**\n",
    "- With {traffic:,} daily visitors: **{days_needed} days**\n",
    "\n",
    "üìà **Parameters:**\n",
    "- Baseline CR: {result.baseline_rate:.2%}\n",
    "- Target CR: {result.target_rate:.2%}\n",
    "- MDE: {result.mde_percentage} pp ({result.mde_absolute:.2%})\n",
    "- Significance: {(1-result.alpha)*100:.0f}%\n",
    "- Power: {result.power*100:.0f}%\n",
    "\n",
    "üí° **Recommendation:**\n",
    "Run the test for at least {days_needed} days to detect a {result.mde_percentage} pp improvement with statistical confidence.\n",
    "\"\"\"\n",
    "        return output\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        return f\"‚ö†Ô∏è Validation Error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "def validate_ab_handler(ctrl_conv, ctrl_total, treat_conv, treat_total):\n",
    "    \"\"\"Handler para valida√ß√£o de teste A/B.\"\"\"\n",
    "    try:\n",
    "        result = StatisticalToolkit.calculate_statistical_significance(\n",
    "            int(ctrl_conv), \n",
    "            int(ctrl_total), \n",
    "            int(treat_conv), \n",
    "            int(treat_total)\n",
    "        )\n",
    "        \n",
    "        emoji = \"‚úÖ\" if result.is_significant else \"‚è≥\"\n",
    "        direction = \"üìà\" if result.is_positive else \"üìâ\"\n",
    "        \n",
    "        output = f\"\"\"{emoji} **A/B Test Results**\n",
    "\n",
    "üìä **Conversion Rates:**\n",
    "- Control: {result.control_rate:.2%} ({ctrl_conv:,} / {ctrl_total:,})\n",
    "- Treatment: {result.treatment_rate:.2%} ({treat_conv:,} / {treat_total:,})\n",
    "\n",
    "{direction} **Uplift:**\n",
    "- Relative: **{result.uplift_relative_pct:+.2f}%**\n",
    "- Absolute: **{result.uplift_absolute_pp:+.2f} pp**\n",
    "\n",
    "üî¨ **Statistical Test:**\n",
    "- p-value: **{result.p_value:.4f}**\n",
    "- z-statistic: {result.z_statistic:.2f}\n",
    "- Status: **{result.to_dict()['interpretation']}**\n",
    "\n",
    "üìä **95% Confidence Interval:**\n",
    "- Lower: {result.ci_95_lower*100:+.2f} pp\n",
    "- Upper: {result.ci_95_upper*100:+.2f} pp\n",
    "\n",
    "üí° **{result.to_dict()['recommendation']}**\n",
    "\"\"\"\n",
    "        return output\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        return f\"‚ö†Ô∏è Validation Error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "def eda_handler():\n",
    "    \"\"\"Handler para EDA autom√°tica.\"\"\"\n",
    "    global current_csv_data\n",
    "    \n",
    "    if not current_csv_data:\n",
    "        return \"‚ö†Ô∏è Please upload a CSV file first.\"\n",
    "    \n",
    "    try:\n",
    "        result = StatisticalToolkit.perform_eda(current_csv_data)\n",
    "        \n",
    "        output = f\"\"\"üìä **Exploratory Data Analysis**\n",
    "\n",
    "### Dataset Overview\n",
    "- **Shape:** {result.shape['rows']:,} rows √ó {result.shape['columns']} columns\n",
    "- **Duplicates:** {result.duplicate_rows} rows\n",
    "\n",
    "### Data Quality\n",
    "\"\"\"\n",
    "        \n",
    "        if result.missing_values:\n",
    "            output += \"\\\\n**Missing Values:**\\\\n\"\n",
    "            for col, info in result.missing_values.items():\n",
    "                output += f\"- {col}: {info['count']} ({info['percentage']:.1f}%)\\\\n\"\n",
    "        else:\n",
    "            output += \"‚úÖ No missing values\\\\n\"\n",
    "        \n",
    "        output += \"\\\\n### Numeric Summary\\\\n\"\n",
    "        for col, stats in list(result.numeric_summary.items())[:5]:\n",
    "            output += f\"\\\\n**{col}:**\\\\n\"\n",
    "            output += f\"- Mean: {stats['mean']:.2f}\\\\n\"\n",
    "            output += f\"- Median: {stats['median']:.2f}\\\\n\"\n",
    "            output += f\"- Std: {stats['std']:.2f}\\\\n\"\n",
    "            output += f\"- Range: [{stats['min']:.2f}, {stats['max']:.2f}]\\\\n\"\n",
    "        \n",
    "        if result.correlations:\n",
    "            output += \"\\\\n### Top Correlations\\\\n\"\n",
    "            sorted_corr = sorted(result.correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "            for pair, corr in sorted_corr[:5]:\n",
    "                output += f\"- {pair}: {corr:.3f}\\\\n\"\n",
    "        \n",
    "        if result.outliers:\n",
    "            output += \"\\\\n### Outliers Detected\\\\n\"\n",
    "            for col, count in result.outliers.items():\n",
    "                if count > 0:\n",
    "                    output += f\"- {col}: {count} outliers\\\\n\"\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "# Criar interface Gradio\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(primary_hue=\"purple\"),\n",
    "    css=\"\"\"\n",
    "    .hero-section {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
